# 과목1. 데이터 이해

## 제1장 데이터의 이해

### 제1절 데이터와 정보

#### 1. 데이터의 유형
데이터는 그 형태에 따라 다음과 같이 분류된다.

- **정성적 데이터**: 저장, 검색, 분석에 많은 비용이 소요되지 않으며, 문자 형태의 데이터(예: 회사 매출 증가율).
- **정량적 데이터**: 정형화된 데이터로 수치, 도형, 기호 등의 형태를 가진 데이터(예: 나이, 음주량, 주가 등).

#### 2. 지식경영의 핵심 이슈

| 구분  | 의미  | 예시  | 특징  | 상호작용  |
|------|------|------|------|------|
| **암묵지** | 학습과 경험을 통해 개인에게 체화되어 있지만 글로 드러나지 않는 지식 | 김장김치 담그기, 자전거 타기 | 사회적으로 중요하지만 다른 사람에게 공유되기 어려움 | 공통화, 내면화 |
| **형식지** | 문서나 매뉴얼처럼 형상화된 지식 | 교과서, 비디오, DB | 전달과 공유가 용이함 | 표출화, 연결화 |

암묵지는 **경험과 학습을 통한 체득적 지식**이며, **공동화(Socialization)** 나 **내면화(Internalization)** 를 통해 전수됨.  
형식지는 **문서화된 지식**으로, **표출화(Externalization)** 와 **연결화(Combination)** 과정을 통해 공유 가능.

#### 3. DIKW 피라미드
지식 피라미드는 **데이터(Data) → 정보(Information) → 지식(Knowledge) → 지혜(Wisdom)** 의 단계로 구성된다.

##### 1) 데이터(Data)
- 데이터는 가공되지 않은 원시 자료로, 개별적인 사실이나 수치를 의미한다.
- 예: "홍길동의 키는 180cm이다."

##### 2) 정보(Information)

- 데이터에 가공이 더해져 의미를 가지게 된 것.
- 예: "홍길동의 키는 한국 남성 평균보다 10cm 크다."

##### 3) 지식(Knowledge)
- 상황을 분석하고 패턴을 이해하여 유의미한 결론을 도출하는 단계.
- 예: "홍길동의 키는 특정 스포츠에서 유리할 수 있다."

##### 4) 지혜(Wisdom)
- 관련 분야에 대한 전문 이해를 바탕으로 통찰을 제공하는 단계.
- 예: "홍길동은 농구 선수로 적합하다."

### 제2절 데이터베이스의 정의와 특징

#### 1. 데이터베이스 정의
데이터베이스는 체계적으로 조직되고 정리된 자료로, 다양한 사용자와 시스템이 접근하여 활용할 수 있는 독립된 저장소를 의미한다.

#### 2. 데이터베이스 특징

| 특성 | 설명 |
|------|------|
| **통합된 데이터 (Integrated Data)** | 동일한 내용의 데이터가 중복되어 있지 않다는 것을 의미 |
| **저장된 데이터 (Stored Data)** | 자기 디스크나 자기 테이프 등의 보조기억장치에 저장할 수 있는 자료이며 저장되는 값은 데이터베이스의 기본적으로 취할 기록을 바탕으로 함 |
| **공용 데이터 (Shared Data)** | 데이터가 여러 사용자 또는 여러 애플리케이션들에 공통으로 이용된다는 것을 의미 |
| **변화하는 데이터 (Changeable Data)** | 데이터베이스에서 저장된 내용은 곧 데이터베이스의 외적 환경에 의해 지속적으로 나타남 |

### 제3절 데이터베이스 활용

#### 1. OLTP vs OLAP 비교
| 구분 | OLTP | OLAP |
|------|------|------|
| **데이터 구조** | 복잡 | 단순 |
| **데이터 갱신** | 실시간 | 주기적 |
| **응답 시간** | 수 초 이내 | 수 초~수 분 |
| **데이터 범위** | 최신 데이터 | 장기 데이터 저장 |
| **데이터 성격** | 정규화된 핵심 데이터 | 분석용 데이터 |
| **데이터 크기** | GB 단위 | TB 단위 |
| **데이터 특성** | 트랜잭션 중심 | 주제 중심 |
| **엑세스 빈도** | 높음 | 보통 |
| **질의 예측 가능성** | 주기적, 예측 가능 | 예측 어려움 |

**→ OLTP는 운영 데이터 처리에 적합, OLAP는 분석 및 의사결정 지원에 최적화.**

#### 2. 분야별 데이터베이스 소개

| 개념 | 요약 |
|------|------|
| **ERP (Enterprise Resource Planning)** | 경영자원을 하나로 통합하여 기업의 생산성을 극대화하는 시스템. |
| **BI (Business Intelligence)** | 데이터를 분석하여 의사결정을 지원하는 정보 관리 시스템. |
| **CRM (Customer Relationship Management)** | 고객 정보를 분석·통합하여 맞춤형 마케팅을 수행하는 시스템. |
| **EAI (Enterprise Application Integration)** | 기업 내 여러 애플리케이션을 연계하여 정보를 중앙에서 통합·관리하는 기술. |
| **EDW (Enterprise Data Warehouse)** | 데이터를 통합하여 분석·활용할 수 있도록 지원하는 대규모 데이터 저장소. |
| **KMS (Knowledge Management System)** | 기업 지식을 축적·공유하여 경영 효율성을 높이는 시스템. |
| **RFID (Radio Frequency Identification)** | 무선 주파수를 이용해 물품이나 자산을 자동으로 식별하는 기술. |
| **RTE (Real-Time Enterprise)** | 기업 내 모든 자원을 통합하여 실시간으로 의사결정을 지원하는 시스템. |

## 제2장 데이터의 가치와 미래

### 제1절 빅데이터의 이해

#### 1. 빅데이터의 정의
빅데이터는 기존의 데이터와 비교할 수 없을 정도로 방대한 양의 데이터를 의미하며, 다양한 관점에서 정의될 수 있다.

##### 1) 관점에 따른 정의
- **규모적 측면** : 일반적인 데이터베이스 소프트웨어로 저장·관리·분석이 어려운 방대한 데이터
- **비용 및 기술적 측면** : 다양한 형태의 대량 데이터를 생성, 수집, 저장, 분석하는 기술 및 프로세스

##### 2) 3V와 4V

| 3V 개념 | 설명 | 예시 |
|------|------|------|
| **양(Volume)** | 데이터의 규모 측면 | 센서 데이터, 비정형 데이터 (텍스트, 로그, 영상 등) |
| **다양성(Variety)** | 데이터의 유형과 소스 측면 | 정형 데이터 (DB, 스프레드시트), 비정형 데이터 (이미지, 소셜 미디어) |
| **속도(Velocity)** | 데이터의 수집과 처리 속도 | 실시간 스트리밍 데이터, 금융 거래 데이터, SNS 데이터 |

- **4V** : 3V + 가치(Value)

#### 2. 빅데이터 정의의 범주 및 효과

##### 1) 데이터 변화 : 양 (Volume), 종류 (Variety), 속도 (Velocity)

##### 2) 기술 변화 : 데이터 저장, 전송, 분석 기술의 발전, 새로운 데이터 분석 기법 도입

##### 3) 인재, 조직 변화 : **Data Scientist** 같은 새로운 직업 등장

#### 3. 출합 배경과 변화
- **데이터 기술의 발전** : 저장, 네트워크, 분석 기술이 데이터 활용의 가치를 높임
- **인터넷 및 모바일 확산** : 데이터 생성 및 소비 증가
- **다양한 산업에서의 활용** : 기업 경영, 시장 예측, 의사 결정 등 다양한 영역에서 빅데이터 적용 확대

#### 4. 빅데이터에 거는 기대와 비유적 표현

| 개념 | 설명 | 예시 |
|------|------|------|
| **산업혁명의 석탄, 철** | 산업과 서비스 분야의 생산성 향상 기대 | 제조업, 자동화 시스템 |
| **21세기의 원유** | 정보 활용을 통한 산업 전반의 생산성 향상 | AI, 데이터 마이닝 |
| **렌즈** | 데이터가 산업 발전에 미치는 영향 확대 | Ngram Viewer |
| **플랫폼** | 유무형 구조물로 다양한 서드 파티 비즈니스 지원 | 카카오톡, 페이스북 |

#### 5. 빅데이터가 만들어 내는 본질적인 변화

| 기존 방식 | 변화된 방식 | 설명 |
|------|------|------|
| **사전처리** | **사후처리** | 필요한 정보만 수집하던 방식에서 모든 데이터를 저장하고 다양한 방법으로 조합하여 숨은 정보를 발굴. |
| **표본조사** | **전수조사** | 데이터 수집 비용 감소와 클라우드 발전으로 표본조사에서 전수조사 방식으로 변화. |
| **질** | **양** | 데이터가 지속적으로 증가하며, 전체적인 양이 증가할수록 긍정적인 결과를 도출할 가능성이 커짐. |
| **인과관계** | **상관관계** | 특정 현상과 연관된 데이터를 분석하여 예측 가능성을 높이고, 미래 예측이 더욱 정교해지는 변화. |

### 제2절 빅데이터의 가치와 영향

#### 1. 빅데이터의 가치 산정이 어려운 이유

| 요인 | 설명 |
|------|------|
| **데이터 활용방식** | 데이터의 재사용과 다목적 활용이 증가하면서 특정 데이터의 가치를 산정하기 어려워짐. |
| **새로운 가치 창출** | 기존에 없던 데이터 가치가 창출되며, 그 가치를 객관적으로 측정하기 어려워짐. |
| **분석 기술 발전** | 현재는 가치 없는 데이터라도, 새로운 분석 기법이 등장하면 높은 가치를 가질 가능성이 있음. |

#### 2. 빅데이터의 영향
빅데이터가 미치는 영향은 기업, 공공, 개인 분야에서 다양한 변화를 유도

| **분야** | **영향** | **내용** |
|---------|---------|---------|
| **기업** | 혁신, 경쟁력 제고, 신사업 창출 | 소비자의 행동을 분석하고 시장 변동을 예측하여 새로운 비즈니스 모델을 창출함 |
| **정부** | 혁신, 생산성 증가, 미래전략 | 기상, 교통, 의료 등 공공 데이터 수집을 통해 사회 발전 및 정책 수립을 지원 |
| **개인** | 맞춤형 데이터 활용 | 개인 맞춤형 서비스 제공 및 사용자의 경험 최적화 |

### 제3절 비즈니스 모델

#### 1. 빅데이터 활용 사례
1) **관련 예시 및 내용**
   - **포털**: 사용자의 로그 데이터를 활용한 검색엔진 개선 및 기존 페이지 랭킹 알고리즘 혁신  
   - **유통**: 고객의 구매 패턴 분석을 통해 상품 및 매장 배치 최적화

2) **정부**
   - 실시간 교통정보 수집, 기후 정보 및 소비 서비스 모니터링을 통한 국가 정책 활용

3) **개인**
   - 스마트 서비스 제공을 위한 사용자 행동 분석 및 지역 맞춤 서비스 추천  
   - 카드 사용 패턴 분석을 통한 최적의 ATM 설치 위치 선정

#### 2. 빅데이터 활용 기본 테크닉
| 테크닉 | 내용 | 예시 | 분석 기법 |
|------|------|------|------|
| **연관규칙학습** | 데이터 속 항목 간의 연관성을 분석하여 특정 패턴을 찾는 방법 | 고객이 빵을 구매할 때 버터를 함께 구매하는 경향이 있는가? | 연관규칙 분석(Apriori, FP-Growth) |
| **유형분석** | 데이터를 특징에 따라 그룹으로 분류하는 기법 | 이 사용자는 소비 성향이 유사한 고객 그룹에 속하는가? | 군집 분석(K-평균, DBSCAN, 계층적 군집) |
| **유전자 알고리즘** | 최적의 해결책을 찾기 위해 자연선택과 돌연변이를 활용해 점진적으로 개선하는 알고리즘 | 광고 효과를 극대화하기 위한 최적의 배너 디자인과 색상 조합은 무엇인가? | 최적화 알고리즘(유전자 알고리즘, 진화 전략) |
| **기계학습** | 기존 데이터를 학습하여 새로운 데이터를 예측하는 기술 | 온라인 쇼핑몰에서 사용자가 다음에 구매할 가능성이 높은 상품은 무엇인가? | 머신러닝 예측(Random Forest, 결정 트리, 신경망) |
| **회귀분석** | 변수 간의 관계를 분석하여 하나의 값이 다른 변수에 미치는 영향을 측정하는 방법 | 광고 지출이 매출 증가에 미치는 영향을 예측할 수 있는가? | 회귀 분석(선형 회귀, 다중 회귀, 로지스틱 회귀) |
| **감정분석** | 텍스트 데이터를 분석하여 감정이나 의견을 파악하는 기술 | 최근 출시된 스마트폰에 대한 고객 리뷰는 긍정적인가, 부정적인가? | 자연어 처리(BERT, 감성 사전 기반 분석) |
| **소셜네트워크분석(=사회관계망분석)** | 사람들 간의 관계와 정보 흐름을 분석하는 기법 | 영향력이 높은 고객을 찾아 타겟 마케팅에 활용할 수 있는가? | 네트워크 분석(PageRank, 중심성 분석, 커뮤니티 탐색) |

### 제4절 위기 요인과 통제 방안

#### 1. 위기 요인에 따른 통제 방안

| 변화 방향 | 내용 | 예시 |
|------|------|------|
| **사후 대응 → 선제적 대응** | 기존의 문제 발생 후 조치 방식에서 사전 예방 중심으로 변화 | 개인정보 유출 사고 후 대응 → 실시간 모니터링 및 자동 차단 시스템 도입 |
| **책임 전환 → 권한 기반 책임제 확립** | 사용자의 책임을 강조하는 방식에서 기업과 기관의 책임을 명확히 분배 | 이용자의 보안 설정 책임 강조 → 플랫폼에서 기본 보안 설정 적용 |
| **데이터 오용 → 알고리즘 검토 강화** | 데이터 활용의 투명성을 높이고 알고리즘 검토를 의무화 | AI 채용 시스템의 차별적 결과 검토 및 조정 |

#### 2. 위기 요인에 따른 통제 방안

| 변화 방향 | 내용 | 예시 |
|------|------|------|
| **개인정보 동의 방식 변화 → 사용자의 책임 강화** | 정보 제공 동의보다 사용자의 데이터 보호 책임을 강조 | 이용자가 데이터 관리 설정을 직접 조정하는 기능 제공 |
| **예측 오류 대응 → 책임 원칙 강화 및 보호 장치 마련** | 잘못된 예측으로 인한 피해를 방지하기 위해 보호 장치 마련 | 신용평가 AI의 오류로 대출 거부 시 이의 신청 절차 제공 |
| **데이터 투명성 확보 → 알고리즘 접근성 향상 및 감시 체계 구축** | 데이터 활용의 투명성을 높이고 알고리즘 검토를 의무화 | 공공기관의 AI 결정 과정에 대한 설명 및 감사 보고서 공개 |

### 제5절 미래의 빅데이터

#### 1. 빅데이터 활용의 3요소
- **데이터**: 모든 것의 데이터화 (Datafication)  
- **기술**: 진화하는 알고리즘 및 인공지능  
- **인력**: 데이터 사이언티스트, 알고리즘 전문가

## 제3장 가치 창조를 위한 데이터 사이언스와 전략 인사이트

### 제1절 빅데이터분석과 전략 인사이트

#### 1. 빅데이터 회의론의 원인
- **부적절한 학습 효과**: 고객의 고객관계관리(CRM) → 광고 마케팅, 투자 대비 효과 미흡  
- **부적절한 성공 사례**: 기존 CRM에서 활용된 데이터 분석 성과를 과대 포장  
- **단순히 데이터만으로 포커스를 두지 말고, 분석을 통해 가치를 만드는 것에 집중해야 함**

#### 2. 일차원적인 분석 vs 전략 도출을 위한 가치 기반 분석
##### 1) 산업별 분석 애플리케이션

| 산업 | 일차원적 분석 애플리케이션 |
|------|------------------|
| **금융** | 대출 승인 평가, 부정 거래 탐지, 투자 위험 분석 |
| **소매업** | 고객 구매 패턴 분석, 재고 최적화, 할인 전략 수립 |
| **제조업** | 공급망 관리, 원자재 수급 예측, 품질 이상 탐지 |
| **운송업** | 물류 최적 경로 분석, 차량 유지보수 예측, 배송 시간 최적화 |
| **헬스케어** | 환자 건강 모니터링, 질병 예측, 의료비 절감 모델 |
| **통신** | 사용자 네트워크 사용 패턴 분석, 데이터 트래픽 예측, 요금제 추천 |
| **정부** | 세금 징수 최적화, 범죄 패턴 분석, 도시 교통 흐름 관리 |
| **온라인 서비스** | 검색엔진 최적화, 사용자 맞춤 광고, 콘텐츠 추천 알고리즘 |

##### 2) 전략 도출 가치 기반 분석

- **분석의 실행 범위를 넓고 실질적으로 변화시키고, 전략적 인사이트를 제공하는 방향으로 발전해야 함**  
- **가치 기반 분석을 통해 산업 내 주요 기회를 확보하고, 경영진의 의사결정 지원을 높여야 함**

### 제2절 전략 인사이트 도출을 위한 필요 역량

#### 1. 데이터 사이언스의 의미와 역할

##### 1) 의미
- 데이터 사이언스는 데이터 공학, 수학, 통계학, 컴퓨터공학, 시각화, 해커의 사고방식 등 다양한 전문지식을 종합한 학문.  
- 정형 및 비정형 데이터를 활용하여 의미 있는 정보를 추출하고, 분석뿐만 아니라 효과적으로 구현하고 전달하는 과정까지 포함하는 포괄적 개념.  
- 예시: 인터넷 로그 분석, 감시 카메라 데이터 처리, 휴대전화 사용 패턴 분석 등.

##### 2) 역할

- 데이터 사이언티스트는 비즈니스 성과를 좌우하는 핵심 이슈 해결을 목표로 하며, 분석 결과를 기반으로 의사결정에 기여해야 함.  
- 데이터 분석뿐만 아니라 소통 능력이 중요하며, 분석된 데이터를 효과적으로 전달하는 것이 필수적.  
- 예시: 매출 예측 모델 구축, 사용자 행동 패턴 분석, 고객 이탈률 감소 전략 수립.

#### 2. 데이터 사이언스의 구성 요소
##### 1) 데이터 사이언스의 영역
- **IT 전문성, Analytics, 비즈니스 분석**이 결합되어 데이터 분석 및 활용의 핵심 요소를 형성

##### 2) 데이터 사이언티스트의 요구 역량

- **Hard Skill**
  - 데이터에 대한 이해 및 처리 기술
  - 분석 기법 적용 능력

- **Soft Skill**
  - 데이터 분석 결과 해석 능력
  - 커뮤니케이션 및 스토리텔링

#### 3. 데이터 사이언스: 과학과 인문의 교차점
- 분석 기술뿐만 아니라 **인문학적 요소**가 중요  
- 데이터 처리 능력뿐만 아니라 **스토리텔링, 커뮤니케이션, 창의력, 윤리적 사고, 비판적 사고** 등의 역량이 필요

#### 4. 전략적 관점과 인문학적 부합

| 외부환경의 변화 | 내용 | 예시 |
|------|------|------|
| **컨버전스 → 디버전스** | 단순한 세계관에서 복잡한 세계관으로 변화 | 맞춤형 서비스 확산, 사용자 경험(UX) 강조, 창의적 문제 해결 역량 중요성 증가 |
| **생산 → 서비스** | 비즈니스 중심이 제품 생산에서 서비스로 이동 | 정기 구독 서비스(구독 경제), 공유 경제 플랫폼(예: 에어비앤비, 우버) 증가 |
| **생산 → 시장창조** | 공급자 중심의 기술 경쟁에서 무형자산 중심의 경쟁으로 변화 | 브랜드 스토리텔링 중요성 증가, 문화·감성 마케팅 활성화, 개인화된 사용자 경험 제공 |

- 데이터 분석은 단순한 숫자 분석이 아닌 **사람 중심의 해석과 적용**이 필수적

### 제3절 빅데이터 그리고 데이터 사이언스의 미래

#### 1. 빅데이터의 시대
- 빅데이터 분석은 **선거 결과 예측, 기업 비용 절감, 시간 절약, 매출 증가, 신사업 창출, 내부 의사결정 지원** 등에서 중요한 가치를 제공

#### 2. 빅데이터 회의론을 넘어 가치 패러다임의 변화

| **단계** | **설명** | **예시** |
|---------|---------|---------|
| **Digitalization (디지털화)** | 다양한 데이터를 디지털 형태로 변환 | 문서 스캔 및 OCR(광학 문자 인식), IoT 기기를 통한 데이터 수집 |
| **Connection (연결)** | 데이터를 연계하여 새로운 인사이트 도출 | 스마트홈 시스템에서 기기 간 연동, 의료 데이터 통합을 통한 맞춤형 치료 |
| **Agency (자율성)** | 데이터 기반으로 자율적인 의사 결정이 가능해짐 | AI 챗봇을 활용한 고객 응대, 자율주행차의 실시간 교통 데이터 분석 및 경로 최적화 |

#### 3. 데이터 사이언스와 한계 및 인문학

##### 1) 데이터 사이언스의 한계
- 데이터 분석 결과가 **반드시 기업의 전략적 결론을 내릴 수 있는 것은 아님**  
- 데이터 사이언스를 통해 가치를 창출하려면 **비즈니스 관점에서 적절한 해석이 필요**

##### 2) 데이터 사이언스와 인문학의 결합

- 데이터 분석과 해석 과정에서 **정량적 데이터뿐만 아니라 정성적 요소도 고려해야 함**  
- 새로운 기술이 등장할 때 **인문학적 통찰력을 통해 적용 방향을 정하는 것이 중요**

# 과목2. 데이터 처리 기술 이해

## 제1장 데이터 처리 프로세스

### 제1절 ETL(Extraction, Transformation and Load)

#### 1. ETL (Extraction, Transformation, and Load)

##### 1) ETL 개념
- **데이터 이동 및 변환 절차와 관련된 업계 표준 용어**
- **운영 데이터 스토어(ODS), 데이터 웨어하우스(DW), 데이터 마트(DM) 등에 데이터를 적재하는 핵심 과정**
- 다수 시스템 간 대용량 데이터 교환 및 복잡도가 높은 **비즈니스 룰을 적용한 데이터 교환**을 수행.  
- ETL 구현을 위한 다양한 상용 소프트웨어가 존재하며, **Batch(일괄) ETL**과 **Real-Time(실시간) ETL**로 구분.  
  - **Batch ETL**: 일정한 주기로 대량의 데이터를 처리하는 방식 (예: 매일 밤 고객 거래 내역을 DW로 일괄 전송).  
  - **Real-Time ETL**: 실시간 데이터 처리를 필요로 하는 경우 활용 (예: 실시간 금융 거래 데이터 모니터링 및 분석).

##### 2) ETL의 주요 기능

| 기능 | 설명 |
|------|------|
| **Extraction (추출)** | 원본 데이터 소스(Source)에서 데이터 수집 |
| **Transformation (변환)** | 데이터 정제, 변환, 필터링 및 비즈니스 규칙 적용 |
| **Loading (적재)** | 변환된 데이터를 타겟 시스템에 저장 |

##### 3) ETL 작업 단계
- **Interface → Staging ETL → Profiling ETL → Cleansing ETL → Integration ETL → Denormalizing ETL**

#### 2. ODS (Operational Data Store)

##### 1) ODS 개념
- **여러 데이터 원본(Source)에서 데이터를 추출하고 통합하는 데이터베이스**
- **실시간(Real-Time) 또는 근실시간(Near Real-Time) 트랜잭션 데이터 저장 목적**

##### 2) ODS의 구성 단계

| 단계 (Stage) | 설명 (Description) | 예시 (Example) |
|------|------|------|
| **인터페이스 (Interface)** | 원본 데이터에서 데이터 획득 | ERP 시스템에서 거래 데이터를 수집 |
| **데이터 스테이징 (Data Staging)** | 데이터를 저장하고 관리 | 수집된 로그 데이터를 임시 저장소에 적재 |
| **데이터 프로파일링 (Data Profiling)** | 데이터 품질 점검 및 오류 탐색 | 중복 데이터 탐색 및 누락된 값 검출 |
| **데이터 클렌징 (Data Cleansing)** | 데이터 정제 및 오류 수정 | 잘못된 형식의 고객 연락처 데이터 수정 |
| **데이터 통합 (Data Integration)** | 다양한 원본 데이터를 일관된 형식으로 통합 | 여러 데이터베이스에서 수집한 고객 정보를 하나의 테이블로 통합 |
| **익스포트 (Export)** | 정제된 데이터를 ETL 프로세스로 전달 | 정제된 데이터를 데이터 웨어하우스로 적재 |

#### 3. 데이터 웨어하우스

##### 1) 데이터 웨어하우스란?
- **ODS에서 정제 및 통합된 데이터를 저장하여 분석 및 보고를 지원하는 데이터 저장소**

##### 2) 데이터 웨어하우스의 특징

| 특징 | 설명 |
|------|------|
| **주제 중심성** | 특정 주제별로 데이터를 조직화 |
| **영속성, 비휘발성** | 한 번 저장된 데이터는 변경되지 않음 |
| **통합성** | 다양한 원천 데이터를 일관성 있게 통합 |
| **시계열성** | 일정한 주기로 데이터가 축적됨 |

##### 3) 테이블 모델링 기법

| 구분 | 스타 스키마 | 스노우 플레이크 스키마 |
|------|------------|----------------------|
| **특징** | 조인 스키마가 단순하며, 성능 최적화 | 테이블 간 정규화된 구조 |
| **장점** | 쿼리 작성이 단순하고 성능이 빠름 | 데이터 중복이 적어 저장 공간 절약 |
| **단점** | 데이터 중복이 많아 스토리지 소모 증가 | 조인 과정이 복잡해 쿼리 성능 저하 |

### 제2절 CDC(Change Data Capture)

#### 1. CDC 개념과 특징

##### 1) 개념
- **데이터베이스 내 데이터 변경을 식별하여 자동으로 추적하는 기술**
- **데이터 이동, 전송, 공유를 효과적으로 수행하는 설계 기법**

##### 2) 특징

- **실시간 또는 근실시간 데이터 통합을 지원**
- **데이터 웨어하우스 및 기타 저장소 구축에 필수적으로 활용됨**

#### 2. CDC의 구현 개념

| 구현 요소 | 설명 |
|------|------|
| **Time Stamp on Rows** | 행(row) 단위로 타임스탬프 적용 |
| **Version Numbers on Rows** | 행의 버전 번호 부여 |
| **Status on Rows** | 행의 상태 정보 관리 |
| **Time/Version/Status on Rows** | 행 단위의 시간, 버전, 상태 종합 관리 |
| **Triggers on Tables** | 테이블에 트리거를 적용하여 변경 감지 |
| **Event Programming** | 이벤트 기반으로 데이터 변경 사항 반영 |
| **Log Scanner on Database** | 데이터베이스 로그 스캐닝을 활용한 변경 사항 추적 |

#### 3. CDC 구현 방식

| 방식 | 설명 |
|------|------|
| **푸시(PUSH) 방식** | 데이터 원본(Source)에서 직접 변경 사항을 식별하여 대상(Target)에 즉시 반영 |
| **풀(PULL) 방식** | 대상 시스템(Target)이 원본(Source)을 정기적으로 스캔하여 필요한 데이터를 가져오는 방식 |

### 제3절 EAI(Enterprise Application Integration)

#### 1. EAI 개념과 특징

##### 1) 개념
- **기업 내 여러 정보 시스템을 연계 및 통합하는 소프트웨어 및 정보 시스템 아키텍처 프레임워크**
- **비즈니스 프로세스를 중심으로 이질적인 애플리케이션 간 원활한 데이터 통합 지원**
- **ETL과 배치 프로세스보다는 실시간 또는 근실시간 데이터 통합을 중점으로 함**

##### 2) 특징

- **이기종 시스템 간 데이터 연계를 가능하게 하는 핵심 솔루션**
- **기업 내 애플리케이션 간 호환성을 높이고 데이터 흐름을 원활하게 함**

#### 2. EAI의 데이터 연계 방식: Hub and Spoke

##### 1) Hub and Spoke  
- **중앙 허브(Hub)가 모든 데이터를 중계하는 방식**
- **각 애플리케이션이 허브와만 연결되므로 관리가 용이**
- **연계 구조가 단순하고 확장성이 뛰어나며, 모든 노드가 스포크(Spoke) 역할 수행**
- **예시:** 중앙 데이터 허브를 통한 ERP-CRM 연계, 항공사 예약 시스템.

##### 2) 기존 데이터 연계 방식 : Point to Point

- 시스템 간 직접 연결하는 방식으로 복잡성과 유지보수 비용 증가.  
- 다수 노드 존재 시 **N(N-1)/2**개의 연결 발생.  
- **예시:** 개별 부서 시스템 API 연동, 은행-카드사 개별 데이터 교환.

#### 3. EAI 구성 요소

| 구성 요소 (Component) | 설명 (Description) | 예시 (Example) |
|------|------|------|
| **어댑터 (Adapter)** | 각 정보 시스템과 EAI 허브(Engine) 간의 연결성 확보 | ERP 시스템과 CRM 시스템 간 데이터 연동 |
| **버스 (BUS)** | 어댑터를 통해 연결된 시스템 간 데이터 연동 경로 제공 | 기업 내 애플리케이션 간 메시지 전달 |
| **브로커 (Broker)** | 데이터 연동 규칙을 통제하고 조정 | 전사적 시스템 간 데이터 흐름 조정 |
| **트랜스포머 (Transformer)** | 데이터 형식 변환을 담당 | XML 데이터를 JSON 형식으로 변환 |

#### 4. EAI와 ESB 비교

| 구분 | EAI (Enterprise Application Integration) | ESB (Enterprise Service Bus) |
|------|--------------------------------|--------------------------------|
| **연계 방식** | 미들웨어(Middleware)를 이용하여 애플리케이션 중심 연계 | 미들웨어(Bus)를 기반으로 서비스 중심 연계 |
| **통합 대상** | 애플리케이션(Application) | 프로세스(Process) |
| **운영 방식** | 중앙 집중형 통합 관리 | ESB에서 분산 운영 |
| **아키텍처** | 허브(Hub) 기반의 애플리케이션 연계 | 버스(Bus) 기반으로 동적인 연결 구조 |

### 제4절 데이터 연계 및 통합 기법 요약

| 구분 | 일괄(Batch) 통합 | 비동기식 실시간 통합 | 동기식 실시간 통합 |
|------|------|------|------|
| **데이터 처리 방식** | 비실시간 데이터 통합 | 근실시간 데이터 통합 | 실시간 데이터 통합 |
| **대상 데이터 크기** | 대용량 데이터 대상 | 중간 용량 데이터 | 목표 시스템 데이터 처리 가능 시에만 원천 데이터 획득 |
| **데이터 처리 복잡성** | 높은 데이터 조작 복잡성 | 중간 수준의 데이터 조작 복잡성 | 비교적 단순한 데이터 처리 |
| **핵심 기능** | 데이터 추출, 변환, 적재 | 데이터 추출, 변환, 적재 | 단일 트랜잭션 단위 데이터 통합 |
| **추가 기능** | CDC(Change Data Capture), 감사 추적, 웹 서비스/SOA | CDC, Data Pooling 및 DB Streams, 웹서비스/SOA | Single Transaction Integration, 웹서비스/SOA |
| **데이터 저장 및 연계** | 파일 기반 저장 및 연계 | 다수의 원천 및 목표 시스템 간 연계 | 단일 또는 다수의 데이터 원천과 직접 연결 |
| **자동화 수준** | 자동화 도구 및 자체 개발 SW 혼용 | 자동화 도구 및 자체 개발 SW 혼용 | 실시간 연계를 위한 트랜잭션 기반 처리 |

### 제5절 대용량 비정형 데이터 처리

#### 1. 대용량 로그 데이터 수집

##### 1) 개념
- **로그(Log)는 기업에서 발생하는 대표적인 비정형 데이터이며, 용량이 방대하여 분석을 위해서는 고성능 확장성이 요구됨**
- **대용량 데이터의 수집 시스템 활용이 필수적**
- **Apache Flume-NG , 페이스북 Scribe , Apache Chukwa 등**

##### 2) 대용량 비정형 데이터 수집 시스템의 특징
- **초고속 수집 성능과 확장성**  
  - 실시간 대용량 데이터 수집 및 서버 확장 용이

- **데이터 전송 보장 메커니즘**  
  - 분산 파일 시스템, DB, NoSQL 저장  
  - 단계별 신호 주고받기 방식으로 전송 안정성 확보

- **다양한 수집과 저장 플러그인**  
  - 로그, 성능 모니터링, 소셜 데이터 등 다양한 비정형 데이터 지원  
  - 하둡, NoSQL 등 다양한 저장소 플러그인 제공

- **인터페이스 상속을 통한 애플리케이션 기능 확장**  
  - 필요에 따라 인터페이스 확장 및 수정 가능  
  - 비즈니스 요구에 맞춰 시스템 기능 조정 가능
  
---

#### 2. 하둡(Hadoop)의 특징

##### 1) 개념
- **대규모 분산 병렬 처리를 위한 대표적인 맵리듀스(MapReduce) 기반 기술**
- **HDFS(하둡 분산 파일 시스템)를 활용한 고성능 분산 저장 및 처리 환경 제공**

##### 2) 하둡의 핵심 특징

- **선형적 확장성**: 노드 추가를 통해 성능 확장 가능
- **고장 감내성**: 데이터 복제 및 다중 HDFS 노드 활용

##### 3) 하둡 에코시스템 구성
| 기능 | 구성 요소 | 주요 기술 | 설명 |
|------|------|------|------|
| **데이터 수집** | Flume, Kafka, Scribe, Chukwa | Sqoop, Hiho | 실시간 및 배치 데이터 수집, DBMS 연계 데이터 이동 |
| **데이터 저장** | HDFS, Kudu | - | 분산 파일 시스템 및 컬럼 기반 스토리지 |
| **데이터 처리** | MapReduce, Spark | YARN, Mesos | 분산 데이터 배치 및 인메모리 처리 |
| **데이터베이스** | HBase | - | 실시간 데이터 처리를 위한 분산 NoSQL 데이터베이스 |
| **데이터 분석** | Pig, Hive, Tajo, Impala, Presto | Mahout | SQL 기반 분석, 머신러닝 및 스크립트 기반 처리 |
| **데이터 시각화** | Zeppelin | - | 데이터 시각화 및 대시보드 제공 |
| **워크플로우 관리** | Oozie, Azkaban | - | 데이터 처리 작업의 스케줄링 및 관리 |
| **데이터 직렬화** | Avro, Thrift | - | 데이터 직렬화 및 통신 지원 |
| **분산 코디네이터** | Zookeeper | - | 분산 환경에서의 노드 조정 및 상태 관리 |

#### 3. 데이터 연동 기술: 스쿱(Sqoop)

- **Sqoop은 HDFS와 관계형 데이터베이스 간 데이터 연동을 위한 솔루션**
- **Oracle, MySQL, PostgreSQL, SQL Server 등의 JDBC 기반 데이터베이스 연동 지원**
- **대용량 데이터를 효과적으로 HDFS로 가져오는 기능 수행**

#### 4. 대용량 쿼리 기술

- **Apache 드릴(Drill)**
- **Apache 스팅거(Stinger)**
- **샤크(Shark)**
- **타조(Tajo)**
- **임팔라(Impala)**
- **호크(HAWQ)**
- **프레스토(Presto)**

## 제2장 데이터 처리 기술

### 제1절 분산 데이터 저장 기술

#### 1. 분산 파일 시스템
##### 1) 개요
- 여러 서버로 구성된 대규모 클러스터 시스템에서 데이터를 저장하고 빠르게 처리하는 시스템
- 대용량 데이터의 확장성, 신뢰성, 가용성을 보장

##### 2) 구글 파일 시스템 (GFS, Google File System)

- 구글의 대규모 클라우드 서비스 플랫폼 기반 파일 시스템
- 파일을 일정 크기의 청크(chunk)로 나누고, 각 청크를 여러 개의 복제본과 함께 청크 서버에 분산 저장
- GFS의 구성요소: 여러 개의 클라이언트, 하나의 마스터, 여러 개의 청크 서버

##### 3) 하둡 분산 파일 시스템 (HDFS, Hadoop Distributed File System)
- 파일 데이터를 블록(또는 청크) 단위로 나누어 여러 데이터노드에 분산·복제·저장
- 낮은 데이터 접근 지연 시간보다 높은 데이터 처리량에 초점

##### 4) HDFS의 구성 요소
- **네임노드 (NameNode)**: 파일 시스템의 네임스페이스(Name Space) 관리, 마스터 역할 수행
- **데이터노드 (DataNode)**: 슬레이브 노드, 클라이언트로부터 데이터 입출력 요청 처리, 블록을 3중 복제하여 저장
- **보조네임노드**: HDFS 상태 모니터링, 주기적으로 네임 노드의 파일 시스템 이미지를 스냅샷 생성

##### 5) 러스터(Lustre)  
- 고성능 컴퓨팅(HPC)에 최적화된 객체 기반 분산 파일 시스템으로 클라이언트, 메타데이터 서버, 객체 저장 서버로 구성.  
- **Write Back Cache**를 활용한 메타데이터 캐싱 성능 개선 및 **Intent 기반 잠금 프로토콜**을 통한 동시성 제어 지원.  
- 데이터는 객체 저장 서버에 분산 저장되며, 메타데이터 서버가 파일 네임스페이스와 접근 관리 수행.

##### 6) 분산 파일 시스템 비교 (GFS, HDFS, Lustre)

| 구성 요소 | **GFS (Google File System)** | **HDFS (Hadoop Distributed File System)** | **Lustre (병렬 파일 시스템)** |
|------|------|------|------|
| **마스터 노드** | 마스터 (Master) | 네임노드 (NameNode) | 메타데이터 서버 (Metadata Server) |
| **슬레이브 노드** | 청크서버 (Chunk Server) | 데이터노드 (DataNode) | 객체 저장 서버 (Object Storage Server) |
| **메타데이터 관리** | 마스터가 파일 네임스페이스 및 청크 위치 관리 | 네임노드가 블록 정보 및 시스템 상태 모니터링 | 메타데이터 서버가 파일 네임스페이스 관리 |
| **데이터 저장 방식** | 청크 단위 저장 | 블록 단위 저장 | 스트라이핑 방식으로 데이터 분산 저장 |
| **데이터 복제** | 기본 3중 복제 | 기본 3중 복제 | RAID 기반 복제 |
| **통신 방식** | 하트비트(Heartbeat)로 청크 상태 모니터링 | 하트비트(Heartbeat)로 블록 상태 모니터링 | 클라이언트와 직접 데이터 통신 |
| **주요 특징** | 대규모 데이터 저장 및 처리에 최적화, 구글 내부 시스템 사용 | 빅데이터 분석 및 분산 저장을 위한 오픈소스 | HPC(고성능 컴퓨팅) 환경에서 최적화된 병렬 파일 시스템 |
| **주요 활용 사례** | 구글 내부 서비스 (Gmail, Google Drive) | Hadoop 기반 빅데이터 분석 | 슈퍼컴퓨터, 연구기관, 금융 데이터 처리 |

#### 2. 데이터베이스 클러스터
##### 1) 개념
- 하나의 데이터베이스를 여러 개의 서버(또는 가상 서버)에서 운영
- 데이터 통합 시 성능과 가용성을 높이기 위해 데이터베이스 파티셔닝 또는 클러스터링 적용

##### 2) 클러스터링(Clustering)과 파티셔닝(Partitioning)

| 구분 | **클러스터링(Clustering)** | **파티셔닝(Partitioning)** |
|------|------|------|
| **개념** | 여러 개의 서버(노드)를 하나의 단위로 묶어 분산 및 병렬 처리 수행 | 하나의 데이터베이스 또는 테이블을 논리적으로 나누어 저장 |
| **목적** | 성능 확장, 부하 분산, 고가용성 확보 | 데이터 관리 효율성 증대, 쿼리 성능 최적화 |
| **구성 요소** | 여러 개의 물리적 서버, 로드 밸런서, 네트워크 | 단일 서버 내 논리적 파티션, 또는 여러 물리적 파티션 |
| **데이터 저장 방식** | 동일한 데이터를 여러 노드에 복제하거나 샤딩하여 분산 저장 | 특정 기준(범위, 해시, 리스트 등)에 따라 데이터를 나누어 저장 |
| **장점** | 장애 발생 시 자동 복구, 높은 확장성, 부하 분산 가능 | 특정 데이터에 대한 검색 속도 향상, 관리 용이 |
| **단점** | 네트워크 지연, 동기화 비용 증가 가능 | 데이터 조인 시 성능 저하 가능, 설계 복잡성 증가 |
| **예시** | Hadoop, Kubernetes 클러스터, MySQL Cluster | MySQL Partitioning, PostgreSQL Table Partitioning |

##### 3) 데이터베이스 클러스터 유형

| 구분 | **무공유 디스크 (Shared Nothing)** | **공유 디스크 (Shared Disk)** |
|------|---------------------------|---------------------------|
| **특징** | 각 인스턴스(노드)가 독립적인 데이터 및 버퍼를 소유 | 모든 인스턴스(노드)가 동일한 데이터를 공유 |
| **장점** | 노드 확장성 무제한, 성능 확장 용이 | 높은 수준의 장애 복원력(Fault-Tolerance) 제공 |
| **단점** | 장애 발생 시 별도 복구 프로세스 필요 | 클러스터 크기 증가 시 디스크 병목 현상 발생 가능 |
| **사용** | **MySQL, IBM DB2 ICE** | **Oracle RAC** |

##### 4) 데이터베이스 파티셔닝 구현 효과
| 효과 | 설명 |
|------|------|
| **병렬처리** | 파티션 간 병렬 처리를 통해 데이터 검색 및 처리 성능 향상 |
| **고가용성** | 특정 파티션 장애 발생 시 전체 서비스 중단 방지 |
| **성능향상** | 데이터 분할을 통한 선형적 성능 증가 효과 |

#### 3. NoSQL
##### 1) 개념 및 특징
- 비관계형 데이터베이스 관리 시스템
- SQL 계열의 쿼리 언어 사용 가능
- 확장성, 가용성, 높은 성능 제공
- 저장되는 데이터 구조에 따라 Key-Value 모델, Document 모델(JSON, XML 등), Graph 모델, Column 모델로 분류

##### 2) NoSQL의 종류

| 유형 | 설명 |
|------|------|
| **구글 빅테이블** | 구글의 대용량 데이터 저장을 위한 분산 데이터 관리 저장소 |
| **HBase** | 하둡 분산 파일 시스템(HDFS) 기반의 컬럼 기반 분산 데이터베이스 |
| **아마존 SimpleDB** | 아마존의 데이터 서비스 플랫폼, 웹 애플리케이션의 실시간 데이터 처리 지원 |
| **마이크로소프트 SSDS** | 마이크로소프트의 클라우드 데이터베이스 서비스, 엔터티 기반 데이터 모델 |

### 제2절 분산 컴퓨팅 기술

#### 1. MapReduce
##### 1) 개념 및 특징
- 구글에서 분산 병렬 컴퓨팅을 이용하여 대용량 데이터를 처리하기 위한 목적으로 제작한 소프트웨어 프레임워크
- 분할정복 방식으로 **대용량 데이터를 병렬로 처리**할 수 있는 프로그래밍 모델

##### 2) 구글 MapReduce

- **Map**과 **Reduce**라는 2개의 단계로 구분
- 구글에서 대용량 처리에 대한 복잡성을 추상화 시켜서 오직 핵심 기능 구현에만 집중하도록 하기 위해 제작

#### 3. Hadoop MapReduce
- Apache 오픈소스 프로젝트로 구글의 논문을 바탕으로 하여 **자바(Java)** 언어로 구현된 시스템
- MapReduce는 **입력(Input) → 스플릿(Split) → 맵(Map) → 컴바인(Combine) → 셔플(Shuffle) → 정렬(Sort) → 리듀스(Reduce) → 출력(Output)** 의 단계로 실행

#### 2. 병렬 쿼리 시스템
##### 1) 개요
- 직접 코딩을 해야한다는 구글 및 하둡 MapReduce의 한계를 보완하기 위해 개발된 시스템으로 **사용자에게 친숙한 쿼리 인터페이스를 통해 병렬 처리**를 가능하게 함

##### 2) 종류

| 시스템 | 특징 |
|--------|----------------------------------|
| **구글 Sawzall** | 맵리듀스를 추상화한 최초의 스크립트 형태 병렬 쿼리 언어, 사용자가 이해하기 쉬운 인터페이스 제공 |
| **Apache Pig** | 야후에서 개발한 오픈소스 프로젝트로, 하둡 맵리듀스 위에서 동작하는 추상화된 병렬 처리 언어 |
| **Apache Hive** | 페이스북에서 개발한 데이터 웨어하우징 인프라로, 하둡 맵리듀스의 모든 기능을 지원 |

#### 3. SQL on 하둡
##### 1) 개념
- **하둡에 저장된 대용량 데이터를 대화 형식의 SQL 질의를 통해 처리하고 분석하는 기술**
- 실시간 처리라는 측면에서 하둡의 제약사항을 극복하기 위한 시도

##### 2) 임팔라 (Impala)

- SQL on 하둡 기술 중 **대중에게 공개된 최초의 기술**
- 분석과 트랜잭션 처리를 모두 지원하는 것을 목표로 만들어진 **SQL 질의 엔진**
- C++ 언어를 사용하였으며, 맵리듀스를 사용하지 않고 실행 중에 최적화된 코드를 생성하여 데이터를 처리

##### 3) SQL on Hadoop 제품 예시

| 제품명 | 주요 특징 |
|------|------|
| **Apache Hive** | Hadoop 기반 SQL 인터페이스, 배치 쿼리 처리, MapReduce 활용 |
| **Apache Impala** | 실시간 SQL 쿼리 처리, 고속 분석, Hive보다 빠른 응답 속도 |
| **Apache Drill** | 스키마리스(SQL without schema) 방식, 다양한 데이터 소스 지원 |
| **Presto** | 분산 SQL 엔진, 다양한 데이터 소스 지원, 높은 확장성 |
| **Tajo** | 실시간 SQL 분석, 대규모 데이터 웨어하우스 최적화 |
| **Spark SQL** | Apache Spark 기반 SQL 인터페이스, 인메모리 처리 최적화 |

### 제3절 클라우드 인프라 기술

#### 1. 클라우드 컴퓨팅
##### 1) 개념 및 특징
- 클라우드 컴퓨팅은 동적으로 확장할 수 있는 가상화 자원들을 인터넷으로 서비스하는 기술을 의미
- VMware, Xen, KVM 등과 같은 서버 가상화 기술은 IaaS(Infrastructure as a Service)에 주로 활용
- 아마존의 EMR(Electric MapReduce)은 하둡을 온디맨드(On-Demand)로 이용할 수 있는 클라우드 서비스

##### 2) 용어

| 구분 | 설명 |
|------|------|
| **IaaS (Infrastructure as a Service)** | 서버, 네트워크, 스토리지 등 IT 인프라 자원을 빌려주는 서비스 |
| **PaaS (Platform as a Service)** | 애플리케이션 개발 및 실행을 위한 플랫폼을 제공하는 서비스 |
| **SaaS (Software as a Service)** | 웹을 통해 소프트웨어를 제공하여 직접 설치 없이 사용할 수 있는 서비스 |

#### 2. 서버 가상화의 개념 및 특징
##### 1) 개념
- 물리적인 서버와 서버 운영체제 사이에 적절한 계층을 추가해 서버를 사용하는 사용자에게 물리적인 자원은 숨기고 논리적인 자원만을 보여주는 기술

##### 2) 특징

- 서버 가상화는 하나의 서버에서 여러 개의 애플리케이션, 미들웨어, 운영체제들이 서로 영향을 미치지 않으면서 동시에 사용할 수 있도록 해줌
- 서버 가상화를 가능하게 하는 기술은 매우 다양하며 메인프레임, 유닉스 서버, x86 서버 등에 따라 서로 다른 기술이나 분류체계 사용

#### 3. 서버 가상화의 효과
- 가상머신 사이의 데이터 보호
- 예측하지 못한 애플리케이션, 운영체제의 장애로부터 보호
- 공유 자원에 대한 강제 사용의 거부
- 서버 통합: 동일한 데이터센터의 물리적 자원(공간, 전원 등)을 이용하면서 더 많은 서버를 운영 가능
- 가상화된 서버를 추가하지 않고도 테스트 환경을 구성 가능
- 정확하고 안전한 서버 시팅: 필요할 때만큼 가상머신을 할당 가능
- 시스템 관리: 하드웨어 장애 관리, 로드 밸런싱, 업그레이드가 용이

#### 4. CPU 가상화
##### 1) 하이퍼바이저(Hypervisor)
- 호스트 컴퓨터에서 다수의 운영 체제를 동시에 실행하도록 하기 위한 논리적 플랫폼(Platform)을 의미
- 하이퍼바이저를 통해 사용자는 추가 하드웨어 구입 없이 새로운 운영체제의 설치, 애플리케이션의 테스트 및 업그레이드를 동일한 물리적 서버에서 동시에 수행할 수 있음

##### 2) 완전 가상화

- 하이퍼바이저가 게스트 운영체제를 수정 없이 실행하는 방식. 하드웨어의 가상화 기능을 활용하지 않아도 다양한 운영체제를 실행할 수 있지만, 성능 오버헤드가 발생할 가능성이 있음.

##### 3) 하드웨어 지원 가상화  
- Intel VT-x, AMD-V와 같은 하드웨어 가상화 기술을 활용하여 성능을 최적화하는 방식. 하드웨어에서 직접 가상화를 지원하기 때문에 기존 완전 가상화보다 성능이 뛰어나지만, 해당 기능을 지원하는 CPU가 필요함.

##### 4) 반가상화  
- 게스트 운영체제를 일부 수정하여 하이퍼바이저와 직접 소통하도록 하는 방식. 완전 가상화보다 성능이 뛰어나지만, 게스트 운영체제의 수정이 필요하여 모든 OS에서 적용하기 어려울 수 있음.

##### 5) 호스트 기반 가상화  
- 호스트 운영체제 위에서 가상화 소프트웨어를 실행하는 방식으로, 설치 및 설정이 간편하여 테스트 환경에 적합함. 하지만 하드웨어 자원을 공유하기 때문에 성능 저하가 발생할 가능성이 높음.

##### 6) 컨테이너 기반 가상화  
- 운영체제 커널을 공유하며 가상화 환경을 제공하는 방식. 빠른 실행 속도와 낮은 오버헤드가 장점이지만, 호스트 운영체제와 동일한 운영체제만 실행 가능하다는 제한이 있음.

##### 7) 가상화 방식 비교

| 구분 | 설명 | 장점 | 단점 |
|------|------|------|------|
| **완전 가상화** | 하이퍼바이저가 게스트 OS를 수정 없이 실행 | OS 수정 없이 다양한 OS 실행 가능 | 성능 오버헤드 발생 |
| **하드웨어 지원 가상화** | CPU의 가상화 기능을 활용하여 성능 향상 | 성능 최적화 | 하드웨어 지원 필요 |
| **반가상화** | 게스트 OS를 수정하여 하이퍼바이저와 직접 소통 | 성능 향상 | 게스트 OS 수정 필요 |
| **호스트 기반 가상화** | 호스트 OS 위에 가상화 소프트웨어 설치 | 간편한 설정, 테스트 환경에 적합 | 성능 저하, 리소스 공유로 인한 영향 |
| **컨테이너 기반 가상화** | OS 커널을 공유하며 가벼운 가상화 환경 제공 | 빠른 실행, 낮은 오버헤드 | 호스트 OS와 동일한 OS만 실행 가능 |

#### 5. 메모리 가상화: VMware 기법
##### 1) 특징
- VMware 하이퍼바이저의 핵심 모듈은 **VMkernel**이며, VMkernel은 Service Console, 디바이스 드라이버들의 메모리 영역을 제외한 나머지 전체 메모리 영역을 모두 관리하면서 가상머신에 메모리를 할당함
- **물리 메모리 주소**: 실제 메모리의 위치를 나타내며, 하드웨어가 직접 참조하는 주소.  
- **가상 메모리 주소**: 운영체제가 관리하는 논리적 주소로, 32비트 OS에서는 최대 4GB까지 사용 가능.
- VMware는 하이퍼바이저 내에 **Shadow Page Table**을 별도로 두어 가상 메모리 주소와 물리 메모리 주소의 중간 변환 과정을 가로챔

##### 2) 메모리 할당 문제의 해결 방법

| 기법 | 설명 |
|------------|--------------------------------------------------|
| Memory ballooning | 예약된 메모리보다 더 많은 메모리를 사용하는 가상머신의 메모리 영역을 빈 값으로 강제로 채워 가상머신 운영체제가 자체적으로 Swapping 하도록 함 |
| Transparent page sharing | 각 가상머신에 할당된 메모리 중 동일한 내용을 담고 있는 페이지를 물리적인 메모리 영역에 하나만 존재시키고 모든 가상머신이 공유하도록 함 |
| Memory Overcommitment | 2GB 메모리를 가진 물리적 장비에 512MB를 Minimum Reserved로 가질 수 있는 가상머신 5개를 수행할 수 있으며, 성능저하를 유발할 수 있음으로 권장되지 않음 |

#### 6. I/O 가상화
- 가상머신 실행 시 가장 문제가 되는 것은 I/O에서의 병목현상이므로, I/O 자원의 공유 및 파티셔닝이 필요

| 유형 | 설명 |
|------------|--------------------------------------------------|
| 가상 이더넷 | 물리적으로 존재하지 않는 자원을 만들어 내는 에뮬레이션(Emulation) 기능을 이용, 별도의 물리적 이더넷과 케이블을 사용하지 않고도 네트워크 이중화, 네트워크 안정성 단절 등의 효과를 얻을 수 있음 |
| 공유 이더넷 어댑터 | 여러 개의 가상머신이 물리적인 네트워크 카드를 공유할 수 있게 하며, 공유된 물리적 카드를 통해서 외부 네트워크와 통신이 가능 |
| 가상 디스크 어댑터 | 가상화된 환경에서 가상 디스크를 이용해 가상머신이 디스크 자원을 획득하는 방법에는 내장 디스크, 외장 디스크의 두 가지가 있음 <br>- **내장 디스크**: 가상 I/O 레이어가 물리적 디스크를 논리적 드라이브(LUN)로 분할하여 가상 머신에 제공.<br>- **외장 디스크**: 가상 I/O 레이어 없이 직접 가상 머신에 연결되며, 주로 SCSI 인터페이스를 활용.|

# 과목3. 데이터분석 기획

## 제1장 데이터분석 기획의 이해

### 제1절 분석 기획 방향성 도출

#### 1. 분석 기획의 특징
- 분석 기획: **실제 분석을 수행하기 전에 목표와 방향을 정의하여 효과적인 결과를 도출하는 과정**  
- 성공적인 분석 기획을 위해 **문제를 사업적 가치와 연결하는 능력**이 중요

#### 2. 데이터 사이언티스트의 역할
- 데이터 사이언티스트는 **수학/통계학 지식, 정보기술(IT) 기술, 데이터 활용 역량, 비즈니스 도메인 지식**을 갖춰야 함

#### 3. 분석 대상과 방법
- 분석의 주요 목적과 대상에 따라 **4가지 방법으로 분류**

| 분석 방법 (How) \ 분석 대상 (What) | Known (알려진 대상) | Unknown (알려지지 않은 대상) |
|---------------------------------|------------------|------------------|
| **Known (알려진 방법)**  | 최적화 (Optimization) | 통찰 (Insight) |
| **Unknown (알려지지 않은 방법)** | 해결책 (Solution) | 발견 (Discovery) |

#### 4. 목표 시점별 분석 기획 방향
- **단기 분석(1차 목표)**: 빠른 테스트와 해결책 도출이 중요  
- **장기 분석(2차 목표)**: 정확도 향상, 지속적 개선, 문제 정의에 초점

|| 단기 목표 (과제 단위) | 장기 목표 (마스터 플랜 단위) |
|---|---------------------------|-----------------------------|
|1차 목표| 속도 & 테스트 (Speed & Test) | 정확성 & 배포 (Accuracy & Deploy) |
|과제의 유형| 빠른 성과 (Quick & Win) | 장기적 관점 (Long Term View) |
|접근 방식| 문제 해결 (Problem Solving) | 문제 정의 (Problem Definition) |

#### 5. 분석 기획 시 고려사항
| **항목** | **설명** |
|------|------|
| **가용 데이터(Available Data) 고려** | 데이터 확보가 우선이며, 유형에 따라 분석 방법과 솔루션이 달라짐. |
| **적절한 활용방안과 유즈케이스(Proper Business Use Case) 탐색** | 검증된 분석 시나리오와 솔루션을 적극 활용하는 것이 중요함. |
| **장애 요소 사전 계획 수립(Low Barrier Of Execution)** | 실행 장벽을 낮추기 위해 지속적 교육과 변화 관리가 필요함. |

### 제2절 분석 방법론

#### 1. 분석 방법론 개요
##### 1) 기업의 합리적 의사결정을 가로막는 장애 요소
- **고정 관념(Stereotype)**, **편향된 생각(Bias)**, **프레이밍 효과(Framing Effect)**

##### 2) 방법론 생성과정정

| **구분** | **의미** | **예시** | **특징** | **상호작용** |
|------|------|------|------|------|
| **암묵지** | 개인의 경험과 학습을 통해 내재된 지식 | 김치 담그기, 자전거 타기 | 공유하기 어려우나 사회적으로 중요 | 공동화, 내면화 |
| **형식지** | 문서나 매뉴얼 등으로 명확하게 표현된 지식 | 교과서, 비디오, 데이터베이스 | 전달과 공유가 용이 | 표출화, 연결화 |

##### 3) 방법론의 적용 업무의 특성에 따른 모델
- **폭포수 모델(Waterfall Model)**  
- **프로토타입 모델(Prototype Model)**  
- **나선형 모델(Spiral Model)**

#### 2. KDD 분석 방법론
| **단계** | **설명** | **예시** |
|---------|---------|---------|
| **데이터 선택 (Selection)** | 분석할 데이터를 선택하고 필요한 속성을 정의 | 고객 구매 데이터 선택 |
| **데이터 전처리 (Preprocessing)** | 누락값 처리, 이상값 제거 등 데이터 정제 과정 | 결측치 보완 및 데이터 정리 |
| **데이터 변환 (Transformation)** | 데이터의 구조 변환 및 차원 축소 | 범주형 데이터를 수치형으로 변환 |
| **데이터 마이닝 (Data Mining)** | 패턴 발견 및 모델 생성 | 연관 규칙 분석, 군집 분석 |
| **결과 평가 (Interpretation/Evaluation)** | 모델 성능 평가 및 해석 | 모델의 정확도 및 유용성 평가 |

#### 3. CRISP-DM 분석 방법론
| **단계** | **설명** | **예시** |
|---------|---------|---------|
| **업무 이해 (Business Understanding)** | 비즈니스 목표를 정의하고 문제를 이해 | 고객 이탈 예측 목표 설정 |
| **데이터 이해 (Data Understanding)** | 데이터 탐색 및 초기 분석 수행 | 데이터 분포 및 패턴 탐색 |
| **데이터 준비 (Data Preparation)** | 데이터 정제 및 통합 과정 | 변수 선택 및 정규화 수행 |
| **모델링 (Modeling)** | 알고리즘을 적용하여 모델 생성 | 의사결정나무, 회귀 분석 활용 |
| **평가 (Evaluation)** | 모델의 성능을 평가하고 개선 | 테스트 데이터로 모델 검증 |
| **전개 (Deployment)** | 실무 적용 및 모델 운영화 | 추천 시스템 도입 및 활용 |

#### 4. 빅데이터 분석 방법론
##### 1) 빅데이터 분석의 계층적 프로세스
- **Phase → Task → Step** 으로 구성

##### 2) 빅데이터 분석 방법론의 5단계

| **단계** | **설명** | **예시** |
|---------|---------|---------|
| **분석 기획** | 비즈니스 이해 및 목표 정의 | 고객 세분화 전략 수립 |
| **데이터 준비** | 데이터 수집 및 전처리 | 고객 구매 데이터 정제 |
| **데이터 분석** | 분석 수행 및 모델 개발 | 머신러닝 모델을 활용한 예측 분석 |
| **시스템 구현** | 분석 모델을 시스템화 | 추천 시스템 구축 및 운영 |
| **평가 및 전개** | 프로젝트 완료 및 적용 | 모델 성능 평가 및 지속적 개선 |

### 제3절 분석 과제 발굴

#### 1. 분석과제 발굴 방법론
##### 1) 하향식 접근 방식 (Top Down Approach)
- 분석 결과가 주어지고 이에 대한 해법을 찾기 위해 체계적이고 단계적으로 수행하는 방식

##### 2) 상향식 접근 방식 (Bottom Up Approach)

- 문제의 정의 자체가 어려운 경우, 데이터를 기반으로 문제의 지속적인 개선을 유도하는 방식


##### 3) 디자인 사고 (Design Thinking) 
- 발산적 사고(상향식 접근)와 수렴적 사고(하향식 접근)를 반복적으로 수행하며, 보완적 동적 환경을 통해 최적의 의사결정을 도출하는 방식

| 분석 방법 (How) \ 분석 대상 (What) | Known (알려진 대상) | Unknown (알려지지 않은 대상) |
|---------------------------------|------------------|------------------|
| **Known (알려진 방법)**  | ⬇️최적화 (Optimization) | ⬆️통찰 (Insight) |
| **Unknown (알려지지 않은 방법)** | ⬇️해결책 (Solution) | ⬆️발견 (Discovery) |
|  | ⬇️하향식 접근 방법 | ⬆️상향식 접근방법 |

#### 2. 하향식 접근 방식 (Top Down Approach)
- 문제 탐색(Problem Discovery) → 문제 정의(Problem Definition)  
→ 해결방안 탐색(Solution Search) → 타당성 검토(Feasibility Study) 과정으로 이루어짐

| **단계** | **설명** |
|---------|---------|
| 문제 탐색 | 비즈니스 모델 기반 또는 외부 사례 기반으로 문제를 탐색 |
| 문제 정의 | 데이터 분석을 통해 문제를 변환 |
| 해결방안 탐색 | 수행 목표를 도출하여 최적의 해결책을 모색 |
| 타당성 검토 | 타당성 평가 후 최종 과제 선정 |

#### 3. 하향식 접근 방식 (Top Down Approach)의 과정
##### 1) 문제 탐색 (Problem Discovery)
- **비즈니스 모델 기반**: 업무(Operation), 제품(Product), 고객(Customer), 규제/감사(Regulation & Audit), 지원 인프라(IT & HR)  
- **외부 참조 모델 기반**: 유사 사례 탐색 및 벤치마킹 활용

##### 분석기회 발굴의 범위 확장

###### 1. 거시적 관점의 메가트랜드 (STEEP)
| **영역** | **내용** | **예시** |
|---------|---------|---------|
| **사회(Social)** | 고객 확장과 시장 확대를 위한 사회적, 문화적 트렌드 변화 분석 | 노령화, 밀레니얼 세대 등장, 저출산 영향 |
| **기술(Technological)** | 기술 발전에 따른 제품 및 서비스 개발 기회 탐색 | IT 융합, 로봇 기술 발전, AI 혁신 |
| **경제(Economic)** | 경제 구조 변화 및 시장 흐름에 따른 기회 도출 | 원자재 가격 변화, 환율 영향 |
| **환경(Environmental)** | 환경 규제 및 지속 가능성 관련 시장 변화 분석 | 탄소 배출 규제, 친환경 정책 |
| **정치(Political)** | 주요 정책 방향 및 정부 규제 변화 분석 | 대규모 정책 변화, 무역 협정 영향 |

###### 2. 경쟁자 확대 관점

| **영역** | **내용** | **예시** |
|---------|---------|---------|
| **대체재(Substitute)** | 기존 제품을 대체하는 서비스 및 제품 탐색 | 온라인 교육 서비스 증가 |
| **경쟁자(Competitor)** | 주요 경쟁자의 제품·서비스 및 전략 변화 탐색 | 신제품 출시, 경쟁자 가격 조정 |
| **신규 진입자(New Entrant)** | 신규 기업의 시장 진입 및 파괴적 혁신 가능성 분석 | 스타트업의 새로운 비즈니스 모델 |

###### 3. 시장의 니즈 탐색

| **영역** | **내용** | **예시** |
|---------|---------|---------|
| **고객(Customer)** | 고객의 구매 동향 및 제품·서비스 개선 기회 탐색 | 조선 산업 과잉 생산 해소 방안 분석 |
| **채널(Channel)** | 다양한 판매 경로 및 채널 전략 변화 분석 | 온라인 전환 증가, 디지털 마케팅 확대 |
| **영향자(Influencer)** | 기업 의사결정에 영향을 주는 주요 이해관계자 분석 | M&A 증가, 대기업 투자 방향 변화 |

###### 4. 역량의 재해석

| **영역** | **내용** | **예시** |
|---------|---------|---------|
| **내부 역량(Competency)** | 조직이 보유한 자산과 기술을 재해석하여 분석 기회 도출 | 부동산 활용, 데이터 자산 분석 |
| **파트너 & 네트워크(Partners & Network)** | 관계사 및 협력사의 역량을 분석하여 시너지 창출 기회 탐색 | 공급망 최적화, 공동 R&D |

##### 2) 문제 정의 (Problem Definition)

- 비즈니스 문제를 데이터 문제로 변환하는 단계

##### 3) 해결방안 탐색 (Solution Search)
| 분석 기법 및 시스템(How) \ 분석 역량(Who) | 확보 | 미확보 |
|-----------------|------------------|------------------|
| 기존 시스템 | 기존 시스템 개선 활용 | 교육 및 채용을 통한 역량 확보 |
| 신규 도입 | 시스템 고도화 | 전문업체 Sourcing |

##### 4) 타당성 검토 (Feasibility Study)
- 경제적 타당성, 데이터 및 기술적 타당성 분석

#### 4. 상향식 접근 방식 (Bottom Up Approach)
##### 1) 정의
- **기업이 보유한 원천 데이터를 활용하여 통찰력을 얻는 접근법**  
- **모든 분석 수행이 기여할 수 있는 실무적 관점 적용**

##### 2) 상향식 접근법의 특징

- 논리적 단계적 접근보다는 **디자인 사고(Design Thinking)** 중심  
- **WHY → WHAT 접근 방식**으로 문제를 식별하고 최적의 해결책을 도출  
- **데이터 자체를 기반으로 통찰력 도출**, 분석 모델을 활용  
- **유사성, 연관성 중심 접근** → 프로토타입형 접근법

#### 5. 분석과제 정의
- 분석과제 정의 시 **필요한 소스 데이터, 분석 방법, 데이터 입수 및 분석의 난이도, 수행주기, 검증 요소, 상세 분석 과정** 등을 정리해야 함

### 제4절 분석 프로젝트 관리 방안

#### 1. 분석과제 관리를 위한 5가지 주요 영역
- **분석 프로젝트**는 단순한 분석 수행이 아니라 **업무, 품질, 리스크, 의사소통 등 다양한 관리 요소**가 포함됨  
- 데이터 기반 분석 기법을 적용하는 특성상 **5가지 주요 속성**을 고려하여 추가적인 관리가 필요

##### 1) 분석과제 관리의 5가지 속성
1. **Data Size** (데이터 크기)  
2. **Data Complexity** (데이터 복잡성)  
3. **Analytic Complexity** (분석의 복잡성)  
4. **Accuracy & Precision** (정확도 및 정밀도)  
5. **Speed** (처리 속도)

#### 2. 분석 프로젝트의 특성
##### 1) 분석 기획과 수행의 중요성
- **분석 프로젝트는 개별 분석만이 아닌, 전반적인 프로젝트 관리도 포함됨**  
- 분석 기획과 프로젝트 목적이 **정확한 결과 도출에 필수적**

##### 2) 분석가의 역할

- 데이터 영역과 비즈니스 영역을 **유기적으로 이해하고 적용하는 능력 필요**  
- 프로젝트 목적에 맞춰 **정확한 분석 수행이 필수적**

##### 3) 프로젝트 관리 방식
- 분석 프로젝트는 **도출된 결과를 적용하는 분석 자체의 관리와, 정보가 반복적으로 수행되는 프로토타이핑 방식이 결합됨**  
- **애자일(Agile) 프로젝트 관리 방식**이 적용될 수 있음

## 제2장 분석 마스터 플랜

### 제1절 마스터 플랜 수립

#### 1. 마스터 플랜 수립 프레임 워크
- **분석 과제를 대상으로 우선순위를 설정하고, 데이터 분석 구현을 위한 로드맵을 수립**  
- 우선순위 고려 요소와 적용 범위에 따른 분석 프레임워크 활용

##### 1) 우선순위 고려 요소
1. **전략적 중요도**  
2. **비즈니스 성과/ROI**  
3. **기술 적용 수준**

##### 2) 적용 범위/방식 고려 요소
1. **업무 내재화 적용 수준**  
2. **분석 데이터 활용 수준**  
3. **기술 적용 수준**

#### 2. 우선순위 평가를 위한 ROI 관점에서 빅데이터의 핵심 특징
- ROI 관점에서 **빅데이터의 특성(4V)을 고려하여 투자 대비 효과 분석**

##### 1) 3V(Big Data의 기본 속성)
| 요소 | 설명 | 예시 |
|------|------|------|
| **Volume (규모)** | 데이터의 양이 기하급수적으로 증가함 | 유튜브에서 매일 업로드되는 동영상 데이터는 수백 페타바이트(PB) 수준 |
| **Variety (다양성)** | 데이터의 형태와 출처가 다양해짐 | 소셜미디어 텍스트, 이미지, 영상 데이터와 IoT 센서 데이터가 함께 저장됨 |
| **Velocity (속도)** | 데이터 생성과 처리 속도가 매우 빠름 | 금융 거래 시스템에서 초당 수백만 개의 트랜잭션이 발생하고 분석됨 |

##### 2) 4V(Big Data의 확장 개념)
| 요소 | 설명 | 예시 |
|------|------|------|
| **Value (가치)** | 데이터를 분석하여 비즈니스 인사이트를 얻고, 의사결정에 활용함 | 넷플릭스는 사용자 시청 데이터를 분석하여 맞춤형 콘텐츠를 추천하고, 신규 오리지널 콘텐츠 제작 방향을 결정함 |

#### 3. 포트폴리오 사분면 분석을 통한 과제 우선순위 선정
##### 1) 개요
- 과제 우선순위는 **난이도(Difficulty)와 시급성(Urgency)** 을 기준으로 결정됨.  
- 분석 과제를 **4가지 유형**으로 구분하여 적용 우선순위를 설정함.

##### 2) 분석 과제 유형
- **Ⅰ (현재, 어려움)**: 전략적 중요도가 높아 시급하지만 난이도가 높아 중장기 계획 필요.  
- **Ⅱ (미래, 어려움)**: 현재 중요도는 낮지만, 중장기적으로 추진 필요하며 후순위 고려.  
- **Ⅲ (현재, 쉬움)**: 전략적 가치를 가지며 난이도가 낮아 즉각적인 적용 가능.  
- **Ⅳ (미래, 쉬움)**: 중요도는 낮으나 장기적 관점에서 추진 필요, 여유롭게 진행.

##### 3) 우선순위 결정 기준
- **시급성 기준:** Ⅲ → Ⅳ → Ⅰ → Ⅱ 순서로 적용.  
- **난이도 기준:** Ⅲ → Ⅰ → Ⅳ → Ⅱ 순서로 적용.

##### 4) 적용 방안
- 데이터 양, 특성, 분석 범위를 조정하여 우선순위를 변경 가능.  
- **Ⅰ사분면 과제라도 난이도를 낮추면 Ⅲ사분면으로 조정 가능**, 이를 통해 전략적으로 적용 가능.

### 제2절 분석 거버넌스 체계 수립

#### 1. 거버넌스 체계
##### 1) 개요
- 기업에서 **데이터를 활용한 의사결정의 중요성이 증가**함에 따라, 체계적인 데이터 분석 및 관리가 필수적이다.  
- 단순한 대용량 데이터의 **수집·축적보다는 목적에 맞는 데이터 분석 활용이 더욱 중요**하다.  
- 조직 내 **분석 관리체계를 수립하는 이유는 데이터 분석을 기업 문화로 정착시키고, 지속적인 고도화를 추진하기 위함**이다.

##### 2) 데이터 분석 거버넌스 체계 구성 요소

| 구성 요소 | 설명 |
|-----------|--------------------------------|
| **분석 조직 (Organization)** | 분석 기획 및 관리 수행을 담당하는 조직적 구조 |
| **운영 프로세스 (Process)** | 과제 기획 및 운영을 위한 절차와 워크플로우 정의 |
| **분석지원 시스템 (System)** | 데이터 분석을 지원하는 플랫폼, 도구 및 기술 환경 |
| **데이터 (Data)** | 분석에 활용되는 데이터의 수집, 저장, 품질 관리 체계 |
| **인적 자원 (Human Resource)** | 분석 관련 교육 및 마인드 육성을 위한 역량 강화 체계 |

#### 2. 데이터 분석 수준 진단
##### 1) 분석 준비도 (Readiness)
- 기업의 데이터 분석 도입 및 실행 준비 수준을 평가하는 것.

| **구성 요소** | **주요 내용** |
|--------------|--------------------------------|
| **분석 업무 파악** | 발생 사실 분석, 예측·시뮬레이션 분석, 최적화 분석, 정기적 개선 |
| **인력 및 조직** | 분석 전문가 및 교육, 관리자의 분석 능력, 전사 분석 총괄 조직, 경영진 분석 역량 |
| **분석 기법** | 업무별 적합한 분석 기법 활용, 표준 분석 기법 도입, 효과성 평가 및 정기 개선 |
| **분석 데이터** | 데이터 충족성·신뢰성·적시성 확보, 비구조적 데이터 관리, 외부 데이터 활용, 기준 데이터 관리(MDM) |
| **분석 문화** | 데이터 기반 의사결정, 관리자의 데이터 중심 사고, 협업 및 공유 문화 |
| **IT 인프라** | 운영 시스템 데이터 통합, EAI·ETL 데이터 유통 체계, 분석 전용 서버, 빅데이터·통계·비정형 분석 환경 |

##### 2) **분석 성숙도 모델 (Maturity Model)**

###### 1. 조직의 성숙도 평가 도구* 
- **CMMI(Capability Maturity Model Integration) 모델** :
  - 소프트웨어 개발 및 전산장비 운영 업체들의 업무 능력 및 조직의 성숙도를 평가하기 위한 모델.  
  - 조직이 프로세스를 얼마나 체계적으로 운영하는지 측정하는 기준이 됨.

###### 2. 분석 성숙도 단계별 특징
| **단계** | **도입 단계** | **활용 단계** | **확산 단계** | **최적화 단계** |
|----------|-------------|-------------|-------------|-------------|
| **설명** | 분석 환경 및 시스템 구축 | 분석 결과의 업무 적용 | 전사적 분석 관리 및 공유 | 분석 고도화를 통한 혁신 |
| **비즈니스** | 실적 분석, 정기보고, 운영 데이터 활용 | 예측 분석, 시뮬레이션 | 실시간 분석, 프로세스 혁신, 분석 자원 관리 | 실시간 최적화, 비즈니스 모델 발전 |
| **조직·역량** | 일부 부서 중심, 담당자 역량 의존 | 전문 부서 운영, 관리자가 분석 수행 | 전사 분석 조직 운영, 데이터 사이언티스트 확보 | 데이터 사이언스 그룹, 경영진 분석 활용 |
| **IT 인프라** | 데이터 웨어하우스, ETL, OLAP | 실시간 대시보드, 통계 분석 환경 | 빅데이터 관리, 분석 서버 구축 | 분석 협업 환경, 프로세스 내재화 |

##### 3) **분석 관점에서의 사분면 분석**

|                | **준비도 낮음** | **준비도 높음** |
|--------------|--------------|--------------|
| **성숙도 높음** | **정착형** <br> 준비도는 낮으나, 조직·인력·분석 업무·분석 기법 등이 일부 적용되어 있어 1차적으로 정착이 필요한 기업 | **확산형** <br> 필요한 분석 요소를 갖추고 있으며, 현재 부분적으로 도입되어 지속적인 확산이 필요한 기업 |
| **성숙도 낮음** | **준비형** <br> 기업에 필요한 데이터, 인력, 조직, 분석 업무, 분석 기법 등이 적용되지 않아 사전 준비가 필요한 기업 | **도입형** <br> 분석 업무와 기법은 부족하지만, 적용 조직 등 준비도가 높아 바로 도입할 수 있는 기업 |

#### 3. 데이터 거버넌스 체계 수립
##### 1) 데이터 거버넌스 개요
- **전사 차원의 데이터 정책 및 지침, 표준화, 운용 지침을 포함하는 관리 체계**  
- **프레임워크(Framework)와 저장소(Repository) 기반 운영**  
- **마스터 데이터, 메타 데이터, 데이터 사전 관리 포함**

#### 4. 데이터 거버넌스 구성 요소
- **원칙(Principle), 조직(Organization), 프로세스(Process)로 구성**

##### 1) 데이터 거버넌스 체계
- **데이터 표준화, 관리 체계 구축, 저장소 관리(Repository), 표준화 활동 포함**

##### 2) 데이터 분석을 위한 3가지 조직 구조
| 구분 | **집중 구조** | **기능 구조** | **분산 구조** |
|------|-------------|-------------|-------------|
| **구성** | 별도 분석 전담 조직 운영 | 각 부서에서 분석 수행 | 분석조직 인력을 현업 부서에 배치 |
| **특징** | 분석업무를 전담 조직에서 수행하며 전략적 중요도에 따라 우선순위 조정 가능 | 부서별 분석 수행으로 개별 부서에 최적화되나, 전사 핵심 분석 수행이 어려움 | 현업 부서 중심으로 분석 수행, 신속한 대응 및 Action 가능 |
| **장점** | 분석업무의 이중화/이원화 가능성 높음 | 부서별 맞춤형 분석 수행 가능 | 전사 차원의 분석 수행이 용이, 베스트 프랙티스 공유 가능 |
| **단점** | 업무 부서와의 협업 필요, 즉각적인 반영 어려움 | 전사적 분석 수행 어려움, 부서 간 협업 부족 | 역할 분담이 명확해야 하며, 업무 과다 시 이원화 가능성 존재 |

# 과목4. 데이터분석

## 제1장 R기초와 데이터 마트

### 제1절 R기초

#### **1. R 소개**  
- R은 **오픈소스 프로그램**으로 통계, 데이터 마이닝, 그래픽 분석을 지원.  
- 최신 통계 분석 및 마이닝 기능 제공.  
- 글로벌 커뮤니티를 통한 다양한 예제 공유.  
- 패키지가 수시로 업데이트됨.

#### 2. 편리한 기능
- R 작업환경 설정: `setwd("작업디렉토리")`
- 도움말 확인: `help(함수명)`, `?함수명`
- 실행 취소: `Ctrl + L`

#### 3. 스크립트 사용하기
- **한줄 실행:** `Ctrl + R`
- **다중 실행:** 드래그 후 `Ctrl + R`
- **주석:** `#`

#### 4. 패키지
- **패키지 설치:** `install.packages("패키지명")`
- **패키지 로드:** `library(패키지명)`

#### 5. 변수 다루기
- 변수를 선언하면 자동으로 데이터 타입을 인식
- `print()` 사용 없이도 변수 출력 가능
- **변수 삭제:** `rm(변수명)`

#### 6. 기본적인 통계량 계산

| 기능 | 함수 |
|------|------|
| 평균 | `mean()` |
| 중앙값 | `median()` |
| 표준편차 | `sd()` |
| 분산 | `var()` |
| 공분산 | `cov()` |
| 상관계수 | `cor()` |

#### 7. 함수의 생성 및 활용
- 함수 정의: `function(매개변수) { 실행 코드 }`
- 함수 반환: `return(값)`

#### 8. 연산자 우선순위

| 연산자 | 뜻 | 사용 예시 |
|--------|----|---------|
| `[[` | 인덱스 | `a[1]` |
| `$` | 요소 할당 | `a$coef` |
| `^` | 지수 | `5^2` |
| `+ -` | 단항 부호 | `-3, +5` |
| `%% %/%` | 특수 연산자 | `%% 나머지 연산` |
| `* /` | 곱하기, 나누기 | `3*5` |
| `+ -` | 더하기, 빼기 | `3+5` |
| `== != > < >= <=` | 비교 | `3 == 5` |
| `&` | 논리 AND | `TRUE & TRUE` |
| `|` | 논리 OR | `TRUE | TRUE` |
| `~` | 공식(formula) | `lm(y~x)` |
| `<- -> <<- ->>` | 대입 연산자 | `a <- 3` |
| `?` | 도움말 | `?lm` |

### 제2절 데이터 마트

#### 1. 데이터 마트
- **데이터 웨어하우스와 사용자 사이의 중간층** 역할을 하며, 하나의 주제 또는 부서 중심의 데이터 웨어하우스로 정의됨.

#### 2. 요약변수와 파생변수

| **구분** | **요약변수** | **파생변수** |
|---------|------------|------------|
| **정의** | 분석에 맞게 종합된 변수 | 특정 기준을 적용하여 생성된 변수 |
| **특징** | 많은 모델이 공통으로 사용 | 특정 상황에서만 유용 |
| **예시** | 기간별 평균, 요약 통계, 결합된 속성 등 | 특정 행동 지표, 사용자 맞춤 변수 등 |

#### 3. reshape 패키지
- **데이터 형태 변형 및 요약을 쉽게 해주는 패키지**
- 핵심 함수:
  - `melt()` : 데이터를 **길게 변형**
  - `cast()` : 데이터를 **원하는 형태로 변환**

##### 1) MYDATA 원본 데이터
| no | day | A1 | A2 |
|----|----|----|----|
| 1 | 1 | 40 | 70 |
| 1 | 2 | 30 | 55 |
| 2 | 1 | 50 | 80 |
| 2 | 2 | 25 | 45 |

##### 2) 데이터 변형 과정

###### 1. melt 함수 사용
- `MD <- melt(MYDATA, id=c("no", "day"))`  
- 데이터 프레임을 길게 변환 (long format)

| no | day | variable | value |
|----|----|---------|------|
| 1 | 1 | A1 | 40 |
| 1 | 2 | A1 | 30 |
| 2 | 1 | A1 | 50 |
| 2 | 2 | A1 | 25 |
| 1 | 1 | A2 | 70 |
| 1 | 2 | A2 | 55 |
| 2 | 1 | A2 | 80 |
| 2 | 2 | A2 | 45 |

##### 3) cast 함수 변형 유형

###### 1. With aggregation (평균 적용)
- `cast(MD, no ~ variable, mean)`

| no | A1 | A2 |
|----|----|----|
| 1 | 35 | 62.5 |
| 2 | 37.5 | 62.5 |

- `cast(MD, day ~ variable, mean)`

| day | A1 | A2 |
|----|----|----|
| 1 | 45 | 75 |
| 2 | 27.5 | 50 |

- `cast(MD, no ~ day, mean)`

| no | day1 | day2 |
|----|----|----|
| 1 | 55 | 42.5 |
| 2 | 65 | 35 |

###### 2. Without aggregation (그룹별 데이터 유지)
- `cast(MD, no + day ~ variable)`

| no | day | A1 | A2 |
|----|----|----|----|
| 1 | 1 | 40 | 70 |
| 1 | 2 | 30 | 55 |
| 2 | 1 | 50 | 80 |
| 2 | 2 | 25 | 45 |

- `cast(MD, no + variable ~ day)`

| no | variable | day1 | day2 |
|----|---------|----|----|
| 1 | A1 | 40 | 30 |
| 1 | A2 | 70 | 55 |
| 2 | A1 | 50 | 25 |
| 2 | A2 | 80 | 45 |

- `cast(MD, no ~ variable + day)`

| no | A1_day1 | A1_day2 | A2_day1 | A2_day2 |
|----|--------|--------|--------|--------|
| 1 | 40 | 30 | 70 | 55 |
| 2 | 50 | 25 | 80 | 45 |

##### 4) 요약
- **melt**: 데이터를 길게 변환 (long format).  
- **cast**: 데이터를 넓게 변환 (wide format).  
- **Aggregation 사용 여부에 따라 데이터 요약 가능**.  
- 다양한 방식으로 데이터를 재구성하여 분석에 활용 가능.

#### 4. sqldf 패키지
- **R에서 SQL 명령을 사용할 수 있도록 지원하는 패키지**
- 주요 기능:
  - `sqldf("SELECT * FROM df LIMIT 6")`
  - `sqldf("SELECT * FROM df WHERE col IN ('BF', 'HF')")`
  - `sqldf("SELECT * FROM df1 INNER JOIN df2 ON df1.key = df2.key")`

#### 5. plyr 패키지
- **데이터를 그룹화하고 변환하는 기능 제공**
- `apply()` 기반으로 데이터 변환을 처리
- `*_ply` 함수들은 입력 데이터 구조에 따라 다양한 형태로 변환 가능함.  
- **첫 번째 글자**: 입력 데이터 유형 (array → "a", data frame → "d", list → "l", etc.).  
- **두 번째 글자**: 출력 데이터 유형 (array → "a", data frame → "d", list → "l", etc.).

|  | **Array** | **Data Frame** | **List** | **Nothing** |
|-----------------|--------|------------|------|----------|
| **Array** | `aaply` | `adply` | `alply` | `a_ply` |
| **Data Frame** | `daply` | `ddply` | `dlply` | `d_ply` |
| **List** | `laply` | `ldply` | `llply` | `l_ply` |
| **N Replicates** | `raply` | `rdply` | `rlply` | `r_ply` |
| **Function Arguments** | `maply` | `mdply` | `mlply` | `m_ply` |

#### 6. data.table 패키지
- **data.table** 패키지는 R에서 가장 많이 사용되는 **데이터 핸들링 패키지** 중 하나.  
- **대용량 데이터 탐색, 연산, 병합에 매우 유용**
- 기존 **data.frame보다 월등히 빠른 속도** 제공.  
- 특정 **Column을 key 값으로 설정하여** 데이터를 효과적으로 처리 가능.  
- **빠른 그룹핑(Grouping), 정렬(Ordering), 짧은 문법 지원** 등에서 **data.frame보다 효율적**.

### 제3절 결측값 처리와 이상값 검색

#### 1. 변수의 구간화
- **신용평가 모델, 고객 세분화 시스템** 등에서 변수에 점수를 적용하기 위해 사용됨.
- **구간화의 원칙:**  
  - 10차 단위를 기준으로 구간화하는 것이 보편적  
  - 5개 구간이 적절하며, 7개 이상은 권장되지 않음

#### 2. 변수 구간화 방법

| 방법 | 설명 |
|------|------|
| **Binning** | 연속형 변수를 범주형 변수로 변환 |
| **의사결정나무** | 분류 목적의 연속형 변수를 범주형 변수로 변환 |

#### 3. 기초 분석 및 데이터 관리

##### 1) 결측값 처리
- **결측값 표현:** `NA`, `99999999`, `Unknown`, `Not Answer` 등  
- **단순 대체(Single Imputation)**  
  1. **완전 분석(Complete Analysis):** 결측값이 있는 행 제거  
  2. **평균 대체(Mean Substitution):** 결측값을 평균, 중앙값으로 대체  
  3. **최근접 이웃 대체(Nearest Neighbor Imputation)**

- **다중 대체(Multiple Imputation)**  
  - 결측값을 **여러 개의 가상 데이터로 대체 후 분석 수행**

#### 4. R의 결측값 처리 관련 함수

| 함수 | 설명 |
|------|------|
| `complete.cases()` | 결측값이 없는 행을 `TRUE`, 결측값이 있는 행을 `FALSE`로 반환 |
| `is.na()` | 결측값 여부를 `TRUE/FALSE`로 반환 |
| `centralImputation()` | NA 값을 평균이나 중앙값으로 대체 |
| `knnImputation()` | 최근접 이웃 알고리즘을 활용하여 대체 |
| `amelia()` | 시계열 데이터에 대한 결측값 보완 |

#### 5. 이상값 처리
##### 1) 이상값
- **이상값은 현실적으로 발생할 수 없거나 오류로 인해 발생한 값**  
- **Bad Data로 간주하여 제거하는 것이 일반적**

##### 2) 이상값 인식
- **ESD (Extreme Studentized Deviation):**  
  - 평균으로부터 **3표준편차 이상**을 이상값으로 간주  
  - 이상값 기준: `data > 평균 + 2.5*표준편차`  
  - **IQR(Interquartile Range) 기준:**  
    - 이상값: `Q3 + 1.5 × IQR` 또는 `Q1 - 1.5 × IQR`

##### 3) 이상값 처리
- **삭제(Truncation):** 이상값을 포함한 레코드 삭제  
- **조정(Winsorizing):** 이상값을 상한 또는 하한 값으로 조정

## 제2장 통계분석

### 제1절 통계학 개론

#### 1. 통계
- 특정집단을 대상으로 수행한 조사나 실험을 통해 나온 결과를 요약해 명확히 표현하는 학문

##### 1) **통계자료의 획득 방법**

| **방법** | **설명** | **예시** |
|----------|---------|---------|
| **전수조사 (Census)** | 모집단의 **모든 개체를 조사**하는 방법으로, 가장 정확하지만 비용과 시간이 많이 소요됨. | 국가에서 **인구총조사**를 실시하여 모든 국민의 정보를 수집. |
| **표본조사 (Sampling)** | 모집단의 일부를 추출하여 조사하는 방법으로, 신속하고 경제적이지만 **표본이 모집단을 대표해야 함**. | 전국 1,000개 기업 중 **100개 회사를 무작위로 선정하여 시장 조사** 수행. |

##### 2) **표본추출 방법**

| **구분** | **설명** | **예시** |
|----------|---------|---------|
| **단순랜덤추출 (Simple Random Sampling)** | 모집단에서 각 개체가 **동일한 확률**로 선택되도록 표본을 무작위로 추출하는 방법. | 학생 1,000명 중 **무작위로 100명을 뽑아** 조사. |
| **계통추출 (Systematic Sampling)** | 모집단을 특정 순서로 정렬한 후, **일정한 간격**으로 표본을 추출하는 방법. | 1,000명의 학생을 가나다순으로 정렬 후, **10번째마다 1명씩 표본 추출**. |
| **군집추출 (Cluster Sampling)** | 모집단을 여러 개의 **군집(Cluster)으로 나누고, 일부 군집을 무작위로 선택하여 전체 조사**하는 방법. | 전국 초등학교 중 **10개 학교를 무작위로 선택한 후, 해당 학교의 모든 학생 조사**. |
| **층화추출 (Stratified Random Sampling)** | 모집단을 특정 기준에 따라 **여러 층(Stratum)으로 나누고, 각 층에서 표본을 무작위로 추출**하는 방법. | 회사 직원 1,000명을 **부서별로 구분한 후, 각 부서에서 10명씩 무작위 표본 추출**. |

##### 3) **자료의 측정 방법**

| **척도** | **설명** | **예시** | **연산 가능 여부** |
|----------|---------|---------|----------------|
| **명목척도 (Nominal Scale)** | 대상의 분류나 속성을 나타내며, 서열이 없음 | 성별(남/여), 혈액형(A/B/O/AB), 국가(한국/미국/일본) | 분류 가능, 크기 비교 불가, 사칙연산 불가 |
| **순서척도 (Ordinal Scale)** | 서열(순위)이 있지만, 간격이 일정하지 않음 | 학년(1학년/2학년/3학년), 고객 만족도(매우 좋음/좋음/보통/나쁨) | 순서 비교 가능, 덧셈·뺄셈 불가, 곱셈·나눗셈 불가 |
| **구간척도 (Interval Scale)** | 서열뿐만 아니라 측정값 간의 간격이 동일 | 온도(°C, °F), IQ 점수, 연도(2000년, 2020년) | 덧셈·뺄셈 가능, 비율 계산 불가, 절대적 0 없음 |
| **비율척도 (Ratio Scale)** | 절대적 0이 존재하며, 비율 비교 가능 | 키(cm), 몸무게(kg), 나이, 소득(원) | 사칙연산 가능, 비율 계산 가능 |

#### 2. 통계분석
| **분류** | **설명** | **예시** |
|----------|---------|---------|
| **기술통계 (Descriptive Statistics)** | 데이터를 요약하고 정리하여 전체적인 경향을 파악하는 분석 방법. | 평균, 표준편차, 최솟값, 최댓값, 히스토그램, 그래프 |
| **통계적 추론 (Statistical Inference)** | 표본 데이터를 이용하여 모집단의 특성을 추론하고 예측하는 방법. | 모수추정, 가설검정, 예측 |

#### 3. 확률 및 확률분포
- **확률변수(Random Variable)**
  - 확률에 따라 나타날 가능성이 확률적으로 주어지는 변수

##### 1) **이산형 확률분포(Discrete Distribution)**

| **이산형 확률분포** | **설명** | **예시** |
|------------------|---------|---------|
| **이항분포 (Binomial distribution)** | 동일한 조건에서 베르누이 시행을 여러 번 반복했을 때 성공 횟수를 나타내는 확률분포 | 5번 타석에서 3번 안타를 칠 확률 |
| **다항분포 (Multinomial distribution)** | 이항분포를 확장하여 세 가지 이상의 결과를 가지는 경우의 확률분포 | 주사위를 여러 번 던졌을 때 각 숫자가 나올 확률 |
| **기하분포 (Geometric distribution)** | 성공할 때까지 시행한 횟수를 나타내는 확률분포 | 3번째 타석에서 첫 안타를 칠 확률 |
| **포아송분포 (Poisson distribution)** | 일정한 시간 또는 공간 내에서 특정 사건이 발생하는 횟수를 나타내는 확률분포 | 한 페이지당 평균 10개의 오타가 있을 때, 한 페이지에서 3개의 오타가 나올 확률 |
| **베르누이 분포 (Bernoulli distribution)** | 단일 시행에서 결과가 두 가지(성공/실패)로만 나타나는 확률분포 | 동전 던지기(앞/뒤), 시험의 합격/불합격 |

##### 2) **연속형 확률분포(Continuous Distribution)**

| **확률분포** | **설명** | **예시** |
|-------------|---------|---------|
| **정규분포 (Normal Distribution)** | 평균을 중심으로 좌우 대칭을 이루며, 대부분의 데이터가 평균 근처에 분포하는 확률분포 | 사람의 키, 시험 점수, 공장 제품의 무게 |
| **t-분포 (t-Distribution)** | 표본 크기가 작을 때(표본 수가 적고 모분산을 모를 때) 사용되는 정규분포의 변형 | 소규모 샘플을 이용한 평균 비교 검정 |
| **카이제곱 분포 (Chi-Square Distribution)** | 분산과 관련된 통계적 검정을 수행할 때 사용되는 분포 | 독립성 검정(두 변수 간의 관계 확인), 적합도 검정(관측값과 기대값 비교) |
| **F-분포 (F-Distribution)** | 두 집단의 분산 비교를 위한 검정에서 사용되는 분포 | ANOVA(분산분석)에서 집단 간 차이 검정 |

#### 4. 추정 및 가설검정
  
| **구분** | **설명** | **예시** |
|----------|---------|---------|
| **점추정 (Point Estimation)** | 모집단의 모수를 하나의 값(점)으로 추정하는 방법 | 표본평균을 이용해 모집단 평균을 추정 |
| **구간추정 (Interval Estimation)** | 신뢰구간을 설정하여 모집단의 모수를 추정하는 방법 | 모평균이 95% 신뢰수준에서 [a, b] 범위에 존재 |
| **가설검정 (Hypothesis Testing)** | 모집단에 대한 가설을 설정하고 표본 데이터를 이용하여 검정하는 과정 | 신약이 기존 약보다 효과가 있는지 검정 |
| **귀무가설 (H₀) vs 대립가설 (H₁)** | 귀무가설(H₀)은 검정하고자 하는 가설의 기본 가정이며, 대립가설(H₁)은 귀무가설이 기각될 경우 채택되는 가설 | "신약과 기존 약의 효과 차이가 없다(H₀)" vs "신약이 기존 약보다 효과가 크다(H₁)" |
| **1종 오류 (Type I Error)** | 귀무가설이 참인데도 기각하는 오류 (α) | 건강한 사람을 병이 있다고 판단 |
| **2종 오류 (Type II Error)** | 귀무가설이 거짓인데도 채택하는 오류 (β) | 실제로 병이 있는 사람을 건강하다고 판단 |

##### 1) 가설검정 결과표

| 정확한 사실 \ 가설검정 결과 | H₀가 사실이라고 판정 | H₀가 사실이 아니라고 판정 |
|--------------------------|------------------|------------------|
| H₀가 사실임 | 옳은 결정 | 1종 오류(α) |
| H₀가 사실이 아님 | 2종 오류(β) | 옳은 결정 |

#### 5. 비모수 검정
- **모집단의 분포에 대한 가정 없이 차이를 검정하는 방법**

| **검정 기법** | **설명** | **예시** |
|--------------|---------|---------|
| **부호 검정 (Sign Test)** | 데이터의 크기 정보 없이 부호(+, -)만을 이용하여 두 집단의 차이를 검정하는 방법 | 신약 투여 전후 체온이 상승(+)했는지 감소(-)했는지 비교 |
| **윌콕슨 순위합 검정 (Wilcoxon’s Rank Sum Test)** | 독립된 두 집단 간 차이를 검정하는 비모수적 방법으로, 데이터의 순위 정보를 이용 | 두 그룹의 시험 점수 차이를 비교 |
| **윌콕슨 부호 순위 검정 (Wilcoxon’s Signed Rank Test)** | 동일한 집단의 사전-사후 차이를 검정하는 방법으로, 순위를 고려하여 차이를 평가 | 운동 프로그램 참여 전후 체중 변화 검정 |
| **만-휘트니 U 검정 (Mann-Whitney U Test)** | 두 독립 집단 간 차이를 비교하는 비모수 검정으로, 윌콕슨 순위합 검정과 유사 | A/B 마케팅 전략 효과 비교 |
| **런 검정 (Run Test)** | 데이터의 무작위성을 검정하는 방법으로, 특정 패턴이 존재하는지를 평가 | 주식 가격 변동이 랜덤한지 여부 검정 |
| **스피어만의 순위상관 검정 (Spearman’s Rank Correlation Analysis)** | 두 변수 간 순위 기반 상관관계를 평가하는 방법 | 학생들의 출석률과 시험 성적 간 관계 검정 |

### 제2절 기초 통계분석

#### 1. 기술 통계
- **기술 통계(Descriptive Statistics):** 자료의 특성을 표, 그림, 통계를 사용해 쉽게 파악할 수 있도록 정리/요약하는 것

##### 1) 통계량에 의한 자료 정리
- **중심 위치의 측도:** 평균, 중앙값, 최빈값
- **산포의 측도:** 분산, 표준편차, 범위, 사분위수범위, 변동계수, 표준오차
- **분포의 형태:** 왜도, 첨도

##### 2) 그래프를 통한 자료 정리

- **범주형 자료:** 막대그래프, 파이차트, 모자이크 플롯
- **연속형 자료:** 히스토그램, 상자그림, 상자그림과 줄기-잎 그림

#### 2. 인과관계의 이해
##### 1) 용어
| **용어** | **설명** | **예시** |
|----------|---------|---------|
| **종속변수 (반응변수, Y)** | 독립변수(X)의 변화에 따라 영향을 받는 변수 | 공부 시간(X)에 따른 시험 점수(Y) |
| **독립변수 (설명변수, X)** | 종속변수(Y)에 영향을 주는 변수 | 광고비(X)에 따른 판매량(Y) |
| **산점도 (Scatter Plot)** | 두 변수 간 관계를 시각적으로 표현하는 그래프 | 키(X)와 몸무게(Y)의 관계 분석 |

| **산점도 분석 질문** | **설명** | **예시** |
|--------------|---------|---------|
| **두 변수 사이의 연관성이 존재하는가?** | 두 변수 간에 일정한 관계(양의 상관관계, 음의 상관관계 등)가 있는지 확인 | 운동량(X)이 증가하면 체중(Y)이 감소하는가? |
| **두 변수 사이의 영향도가 강한가?** | 독립변수(X)가 종속변수(Y)에 미치는 영향의 크기를 평가 | 마케팅 비용(X)이 증가할수록 매출(Y) 증가 폭이 큰가? |
| **이상값의 존재 여부 및 패턴 구분 가능 여부?** | 전체 데이터와 동떨어진 값이 있는지 확인하고, 특정한 패턴이 존재하는지 분석 | 대부분의 데이터와 동떨어진 극단적인 값이 존재하는가? |
   
---

##### 2) 공분산(Covariance)

- 두 변수의 상관 정도를 **공분산을 통해 확인**
- 공식: $` Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)] `$

#### 3. 상관분석(Correlation Analysis)
##### 1) 정의와 특성
- **상관분석:** 두 변수 간의 관계를 상관계수를 이용하여 확인하는 분석 방법
- **상관계수 범위:** -1 ~ +1
  - **양의 상관관계:** +1에 가까울수록 강함
  - **음의 상관관계:** -1에 가까울수록 강함
  - **0에 가까울수록 관계 없음**

##### 2) 상관계수 유형

| 구분 | 피어슨(Pearson) | 스피어만(Spearman) |
|------|---------------|----------------|
| 개념 | 선형적 관계 분석 | 순위 기반의 상관관계 측정 |
| 특징 | 정규성을 가정 | 비모수적 분석, 비선형 관계 가능 |
| 상관계수 값 | 피어슨 계수 $`(\gamma)`$ | 스피어만 계수$`(\rho)`$ |
| R 코드 | `cor(x, y, method="pearson")` | `cor(x, y, method="spearman")` |

### 제3절 다변량 분석

#### 1. 회귀분석의 개요

##### 1) 정의
- 하나 또는 그 이상의 독립변수가 종속변수에 미치는 영향을 추정할 수 있는 통계 기법
- **회귀모형 일반식:**  
  $`
  Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_nX_n + \epsilon
  `$
  - $`Y`$: 종속변수
  - $`X`$: 독립변수
  - $`\beta`$: 회귀계수
  - $`\epsilon`$: 오차항

##### 2) 회귀분석의 검정

- **결정계수 ($`R^2`$)**  
  - 모델의 설명력을 나타내는 지표  
  - $` R^2 = \frac{SSR}{SST} `$
  - 값이 **1에 가까울수록 설명력이 높음**
  
##### 3) 선형회귀 분석의 가정
- **독립성:** 독립변수의 영향이 종속변수에만 미쳐야 함
- **등분산성:** 오차항의 분산이 일정해야 함
- **정규성:** 오차항이 정규분포를 따라야 함
- **비상관성:** 오차항 간에 자기상관이 없어야 함

##### 5) 다중선형회귀분석의 결과 해석

###### 1. 회귀모형의 통계적 유의성 검정 (F-검정)
- 회귀모형의 F-통계량의 **p-value**가 0.05보다 작으면, 회귀모형이 통계적으로 유의하다고 판단.  
- 검정 가설:  
  - 귀무가설 $` (H_0) `$: 모든 회귀계수가 0이다. $` (\beta_1 = \beta_2 = \cdots = \beta_k = 0) `$  
  - 대립가설 $` (H_1) `$: 적어도 하나의 회귀계수가 0이 아니다. $` (\beta_i \neq 0) `$

- **분산분석표 (ANOVA Table)**

| 요인 | 제곱합(SS) | 자유도(df) | 평균제곱합(MS) | 분산비(F) |
|------|----------|----------|--------------|-----------|
| 회귀식 | SSR | k | MSR = $` \frac{SSR}{k} `$ | $` \frac{MSR}{MSE} `$ |
| 오차 | SSE | n-k-1 | MSE = $` \frac{SSE}{n-k-1} `$ | |
| 전체 | SST | n-1 | | |

*(n=관측값 개수, k=독립변수 개수)*

###### 2. 회귀계수의 통계적 유의성 검정 (t-검정)
- 개별 회귀계수의 t-검정을 통해 유의성 판단.  
- 검정 가설:  
  - 귀무가설 $` (H_0) `$: $` i `$번째 회귀계수가 0이다. $` (\beta_i = 0) `$  
  - 대립가설 $` (H_1) `$: $` i `$번째 회귀계수가 0이 아니다. $` (\beta_i \neq 0) `$  
- p-value가 0.05보다 작고, t-통계량의 절댓값이 2보다 크면, 해당 회귀계수는 통계적으로 유의하다고 판단.  
- 표준화된 회귀계수(Standardized Regression Coefficients)를 활용하여 독립변수 간 상대적 영향력 비교.

###### 3. 회귀모형의 설명력 검정 (결정계수 $` R^2 `$ )
- 결정계수 $` R^2 `$ 는 회귀식이 데이터에서 설명할 수 있는 비율을 의미하며, 0~1 사이의 값을 가짐.  
- 높은 결정계수일수록 회귀모형의 설명력이 높음.  
- **다변량 회귀분석**에서는 독립변수의 수가 많아질수록 $` R^2 `$ 가 증가하므로, 수정된 결정계수 $` R^2_{adjusted} `$ 를 활용하여 설명력 평가.

###### 4. 회귀모형의 적합성 검정
- 모형의 잔차를 그래프로 시각화하여 검토.  
- 잔차 분석 및 회귀진단을 수행하여 회귀모형의 적합성을 판단.

#### 2. 회귀분석의 종류

| 분석 유형 | 모델 | 사용 용도 |
|---------|------|---------|
| **단순회귀** | $` Y = \beta_0 + \beta_1X + \epsilon `$ | 한 개의 독립변수를 사용하여 종속변수를 예측할 때 |
| **다중회귀** | $` Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_nX_n + \epsilon `$ | 여러 개의 독립변수를 사용하여 종속변수를 예측할 때 |
| **로지스틱회귀** | $` \log \frac{\pi}{1-\pi} = \beta_0 + \beta_1X_1 + \dots + \beta_nX_n `$ | 종속변수가 범주형(예: 이진 분류)일 때 확률을 예측할 때 |
| **다항회귀** | $` Y = \beta_0 + \beta_1X + \beta_2X^2 + \dots + \beta_nX^n + \epsilon `$ | 비선형 관계를 모델링하기 위해 독립변수의 다항식 항을 추가할 때 |
| **비선형회귀** | $` Y = \alpha e^{\beta X} + \epsilon `$ | 지수, 로그, 시그모이드 등의 비선형 함수를 사용하여 모델링할 때 |

#### 3. 변수 선택법(Variable Selection)

- **목적:** 독립변수의 조합 중 가장 적합한 회귀모형을 선택하는 과정


##### 1) **전진선택법(Forward Selection)**
- **절편만 있는 상수항 모형에서 시작하여 중요한 독립변수를 하나씩 추가**
- **변수가 증가할수록 설명력은 높아지지만 과적합 위험 존재**

##### 2) **후진제거법(Backward Selection)**

- **모든 독립변수를 포함한 후, 기여도가 낮은 변수를 하나씩 제거**
- **설명력이 낮은 변수 제거가 가능하나 중요한 변수를 누락할 위험 있음**

##### 3) **단계별 방법(Stepwise Method)**
- **전진선택법과 후진제거법을 결합**
- **변수를 추가하면서 동시에 필요 없는 변수를 제거하는 방식**
- **가장 균형 잡힌 회귀모델을 찾는 데 유용**

#### 4. 정규화 선형회귀 (Regularized Linear Regression)
- 선형회귀 계수에 대한 **제약 조건**을 추가하여 **과적합(Overfitting)** 문제를 해결하는 방법.
- 계수 크기를 제한하는 방식으로 **Ridge 회귀, Lasso 회귀, Elastic Net 회귀**가 있음.

##### 1) 라쏘 회귀 (Lasso Regression)
- 가중치들의 **절댓값 합**을 최소화하는 것을 제약 조건으로 추가.
- 사용 규제 방식: **L1 규제 (L1 Penalty)**
- 목적 함수:
  $`
  \omega = \arg\min_{\omega} \left( \sum_{i=1}^{N} e_i^2 + \lambda \sum_{j=1}^{M} |\omega_j| \right)
  `$
- 중요하지 않은 가중치는 **완전히 0**으로 만들어 **변수 선택(feature selection) 효과**가 있음.

##### 2) 릿지 회귀 (Ridge Regression)
- 가중치들의 **제곱합(Squared Sum of Weights)** 을 최소화하는 것을 제약 조건으로 추가.
- 사용 규제 방식: **L2 규제 (L2 Penalty)**
- 목적 함수:
  $`
  \omega = \arg\min_{\omega} \left( \sum_{i=1}^{N} e_i^2 + \lambda \sum_{j=1}^{M} \omega_j^2 \right)
  `$
- 계수를 0에 가깝게 줄이지만, 완전히 0으로 만들지는 않음.

##### 3) 엘라스틱넷 (Elastic Net)
- **Lasso 회귀와 Ridge 회귀를 결합**한 모델.
- L1 및 L2 규제를 동시에 적용.
- 목적 함수:
  $`
  \omega = \arg\min_{\omega} \left( \sum_{i=1}^{N} e_i^2 + \lambda_1 \sum_{j=1}^{M} |\omega_j| + \lambda_2 \sum_{j=1}^{M} \omega_j^2 \right)
  `$
- **하이퍼파라미터** $`\lambda_1`$, $`\lambda_2`$ 를 조정하여 L1과 L2 규제 비율을 결정.

### 제4절 시계열 예측

#### 1. 시계열 자료

##### 1) 개요
- **시계열 자료(Time Series):** 시간이 흐름에 따라 관찰된 데이터 집합
- **시계열 데이터 분석 목적**
  - **미래의 값 예측**
  - **추세 분석**
  - **주기 분석**
  - **불규칙성 탐지**

##### 2) 시계열 데이터의 정상성 (Stationarity)
- 평균이 일정 (차분을 통해 정상화)
- 분산이 일정 (변환을 통해 정상화)
- 공분산도 단지 시차에만 의존, 실제 특정 시점 t,s에는 의존하지 않음

##### 3) 시계열 자료 분석 방법
| 분석 방법 | 설명 |
|-----------|------|
| **회귀 분석(계량경제) 방법** | 시간에 따른 변동을 분석하여 미래 값을 예측하는 방법. Box-Jenkins 방법 포함 |
| **Box-Jenkins 방법** | ARMA, ARIMA 모델을 이용하여 시계열 데이터의 패턴을 분석하고 예측 |
| **시계열 분해법(Time Series Decomposition)** | 시계열 데이터를 추세, 계절성, 순환 변동, 불규칙 변동으로 분해하여 분석 |
| **이동평균법(Moving Average Method)** | 일정 기간 동안의 평균을 계산하여 시계열 데이터의 추세를 파악하는 방법 |
| **지수평활법(Exponential Smoothing)** | 과거 데이터의 평균을 활용하되, 최근 데이터에 더 큰 가중치를 부여하여 미래 값을 예측 |

##### 4) 시계열 모형
| 모델 | 설명 |
|------|------|
| **자기회귀 모형 (AR, Autoregressive Model)** | 현재 값이 **과거 값의 평균을 참고하여 결정**되는 모델. 예를 들어, 오늘의 주가는 어제와 그제 주가를 참고해서 결정되는 방식. 과거의 패턴이 계속 이어지는 데이터에서 유용함. |
| **이동평균 모형 (MA, Moving Average Model)** | 현재 값이 **과거의 예측 실수(오차)들을 참고하여 조정**되는 모델. 예를 들어, 주가 예측에서 과거 예측이 많이 틀렸다면, 그 오류를 반영해서 오늘 값을 조정하는 방식. 단기적인 변동 분석에 유용함. |
| **자기회귀이동평균 모형 (ARMA, ARIMA)** | 자기회귀(AR)와 이동평균(MA)을 **합친 모델**. 과거 값과 예측 오류를 함께 고려하여 더 정교하게 예측함. ARIMA는 **추세가 있는 데이터(비정상 시계열)** 도 다룰 수 있도록 차분(differencing)을 추가한 확장 모델. 날씨 예측, 주식시장 분석 등 장기 예측에 많이 사용됨. |

- **ACF (자기상관함수):** 과거 데이터와 현재 데이터 간의 상관성 측정
- **PACF (부분 자기상관함수):** 중간 변수를 제거한 후의 상관성 측정

##### 5) 시계열 자료의 변동 요인

| 요소 | 설명 | 예시 |
|------|------|------|
| **추세요인 (Trend Factor)** | 시간에 따라 변화하는 장기적 패턴 | 전 세계 인구 증가, 물가 상승, 기술 발전으로 인한 스마트폰 보급률 증가 |
| **계절요인 (Seasonal Factor)** | 계절이나 주기적인 요인에 의해 발생하는 반복적인 패턴 | 여름철 아이스크림 매출 증가, 연말 쇼핑 시즌 매출 급증 |
| **순환요인 (Cyclical Factor)** | 경기 변동, 경제적 변화 등으로 인해 발생하는 장기적 변동 요인 | 경기 불황으로 인한 실업률 증가, 호황기 주택 가격 상승 |
| **불규칙요인 (Irregular Factor)** | 예측할 수 없는 변동 요인 (예: 자연재해, 정치적 사건) | 코로나19 팬데믹으로 인한 관광 산업 침체, 대형 지진으로 인한 경제적 타격 |

## 제3장 정형 데이터 마이닝

### 제1절 데이터 마이닝 개요

#### 1. 데이터 마이닝 개요

##### 1) 정의
- **대용량 데이터에서 의미 있는 패턴을 발견하거나 예측하여 의사결정에 활용하는 기법**
- **통계분석과 차이점:**  
  - **통계분석:** 가설을 기반으로 데이터를 검증  
  - **데이터 마이닝:** 다양한 알고리즘을 이용해 숨겨진 패턴 탐색

##### 2) 활용 분야

- **재무, 의료, 유통, 전자상거래 등 다양한 산업에 활용**

##### 3) 주요 기법
- **의사결정나무, 로지스틱 회귀, 최근접 이웃, 연관규칙 분석, 군집분석** 등

#### 2. 분석 방법

| **Supervised Learning (지도학습)** | **Unsupervised Learning (비지도학습)** |
|---------------------------------|--------------------------------|
| 의사결정나무(Decision Tree) | 군집분석(K-Means Clustering) |
| 인공신경망(ANN, Artificial Neural Network) | 연관규칙분석(Association Rule Analysis) |
| 로지스틱 회귀분석(Logistic Regression Analysis) | SOM(Self Organizing Map) |
| 선형 회귀분석(Regression Analysis) | |
| K-최근접 이웃(KNN, K-Nearest Neighbor) | |

#### 3. 데이터 마이닝 추진 단계

| 단계 | 설명 | 예시 |
|------|------|------|
| **목적 설정** | 데이터 마이닝을 수행하는 목적을 명확히 정의하고, 필요한 모델과 데이터를 결정 | 고객 이탈 예측을 위한 데이터 분석 목표 설정 |
| **데이터 준비** | 고객 정보, 거래 내역, 웹로그, 소셜 네트워크 데이터 등 다양한 데이터를 수집 및 정제 | CRM 데이터를 활용하여 고객 행동 패턴 분석 |
| **가공** | 모델링 목적에 맞게 데이터를 변환하고, 마이닝 소프트웨어에 적용할 수 있도록 가공 | 결측치 처리, 범주형 변수를 숫자로 변환 |
| **기법 적용** | 적절한 데이터 마이닝 기법을 적용하여 정보 추출 | 의사결정나무, 신경망, 군집 분석 등 적용 |
| **검증** | 모델 성능을 테스트 데이터와 비교하여 검증하고, 최적의 모델을 선정 및 적용 | 과거 데이터를 사용하여 고객 재구매 예측 모델 평가 |

#### 4. 데이터 분할

| 데이터 유형 | 비율 | 설명 |
|------------|------|-------------------------------|
| **구축용(Training Data)** | 50% | 모델을 학습시키는 데이터 |
| **검증용(Validation Data)** | 30% | 모델의 과적합 여부 평가 |
| **시험용(Test Data)** | 20% | 최종 모델의 성능 평가 |

#### 5. 혼동 행렬 (Confusion Matrix)

##### 1) 혼동 행렬과 성능지표
| 실제 값 \ 예측 값 | 양성 예측 (Positive) | 음성 예측 (Negative) | 성능지표 |
|-----------------|------------------|------------------|---|
| **실제 양성 (Positive)** | TP | FN | 민감도(재현율)  $` (\frac{TP}{TP + FN}) `$ |
| **실제 음성 (Negative)** | FP | TN | 특이도 $` (\frac{TN}{FP + TN}) `$ |
| 성능지표 | 정밀도  $` (\frac{TP}{TP + FP}) `$ | F1 $` (\frac{2TP}{2TP + FP + FN}) `$| 정분류율(정확도) $` (\frac{TP + TN}{TP + TN + FP + FN}) `$ |

##### 2) 혼동 행렬 예시
| 실제 값 \ 예측 값 | 양성 예측 (Positive) | 음성 예측 (Negative) |
|-----------------|------------------|------------------|
| **실제 양성 (Positive)** | 참 양성 (TP) = 10 | 거짓 음성 (FN) = 5 |
| **실제 음성 (Negative)** | 거짓 양성 (FP) = 90 | 참 음성 (TN) = 895 |

##### 3) 성능 지표 (모델 평가 기준)

| 지표 | 계산식 | 값 | 설명 |
|------|---------------------------------|------|------------------------------------------------------------|
| **정확도 (Accuracy, 정분류율)** | $` \frac{TP + TN}{TP + TN + FP + FN} `$ | $` \frac{10 + 895}{10 + 895 + 90 + 5} = 90.5\% `$ | 전체 샘플 중에서 올바르게 분류된 비율 |
| **오분류율 (Error Rate)** | $` 1 - Accuracy `$ | $` 1 - 90.5\% = 9.5\% `$ | 전체 샘플 중에서 잘못 분류된 비율 |
| **민감도 (Sensitivity, TPR), 재현율 (Recall)** | $` \frac{TP}{TP + FN} `$ | $` \frac{10}{10+5} = 67\% `$ | 실제 양성 샘플 중에서 올바르게 예측된 비율 (양성 클래스에 대한 모델의 탐지 능력) |
| **특이도 (Specificity, TNR)** | $` \frac{TN}{FP + TN} `$ | $` \frac{895}{90+895} = 90.9\% `$ | 실제 음성 샘플 중에서 올바르게 예측된 비율 (음성 클래스에 대한 모델의 탐지 능력) |
| **정밀도 (Precision, PPV)** | $` \frac{TP}{TP + FP} `$ | $` \frac{10}{10+90} = 10\% `$ | 예측된 양성 샘플 중에서 실제로 양성인 비율 (예측값의 신뢰도) |
| **F1 점수 (F1 Score)** | $` 2 \times \frac{Precision \times Recall}{Precision + Recall} `$ | $` 2 \times \frac{10\% \times 67\%}{10\% + 67\%} \approx 17\% `$ | 정밀도(Precision)와 재현율(Recall)의 조화 평균 (정확성과 재현율 간의 균형을 평가) |
| **F1 점수 (다른 방식 표현)** | $` \frac{2TP}{2TP + FP + FN} `$ | $` \frac{2(10)}{2(10) + 90 + 5} \approx 17\% `$ | TP, FP, FN만을 사용하여 나타낸 F1 점수 |

#### 6. ROC 패키지로 성과분석
##### 1) ROC Curve (Receiver Operating Characteristic Curve)
ROC 곡선은 모델의 성능을 평가하는 그래프로, **가로축(FPR, False Positive Rate, 1 - 특이도)** 과 **세로축(TPR, True Positive Rate, 민감도)** 으로 구성된다.

##### 2) ROC Curve의 특징
- **이진 분류(Binary Classification)** 모델의 성능을 평가하는 데 많이 사용됨.
- 그래프의 **왼쪽 상단에 가까울수록** 모델의 성능이 우수함.  
  → 올바르게 예측한 비율(TPR)은 높고, 잘못 예측한 비율(FPR)은 낮을수록 좋음.
- **ROC 곡선 아래의 면적(AUROC, Area Under ROC Curve)이 클수록 모델의 성능이 우수**하다고 평가됨.  
  → AUROC 값이 1에 가까울수록 이상적인 모델.

##### 3) AUROC에 따른 모델 평가 기준
| 기준 | 구분 |
|------|------|
| 0.9 - 1.0 | Excellent (A) |
| 0.8 - 0.9 | Good |
| 0.7 - 0.8 | Fair |
| 0.6 - 0.7 | Poor |
| 0.5 - 0.6 | Fail |

#### 7. 모델의 성능 평가
##### 1) 은행의 대출 문제 예제
| 모델 A | 실제 값 \ 예측 값 | 우량 | 불량 |
|-----|-----------------|-----|-----|
|     | **우량 고객 (Positive)** | 70 | 5 |
|     | **불량 고객 (Negative)** | 15 | 10 |

| 모델 B | 실제 값 \ 예측 값 | 우량 | 불량 |
|-----|-----------------|-----|-----|
|     | **우량 고객 (Positive)** | 80 | 5 |
|     | **불량 고객 (Negative)** | 10 | 5 |

##### 2) 기대 수익 계산
- 우량 고객 1명당 기대 수익: **20만원**
- 불량 고객 1명당 손실: **100만원**
- 기대수익(A)
  $` (70 \times 20) + (15 \times -100) = 1400 - 1500 = -100 `$  
- 기대수익(B)
  $` (80 \times 20) + (10 \times -100) = 1600 - 1000 = 600 `$

##### 3) 결론
- **모델 A는 $`-100`$ 만원의 손실, 모델 B는 $`600`$ 만원의 이익**  
- 기대 수익과 대출 리스크를 고려할 때 **모델 B가 더 우수한 성능**을 가짐.

### 제2절 분류분석(Classification)

#### 1. 분류분석과 예측분석

##### 1) 개요
- **분류:** 레코드의 특성과 일관된 패턴을 기반으로 특정 그룹에 속할 확률 예측
- **예측:** 과거 데이터를 바탕으로 미래의 연속적인 값 예측

##### 2) 주요 분류 모델

| 분류 기법 | 설명 | 예시 |
|-----------|---------------------------------------------|------------------------------------------|
| **로지스틱 회귀분석 (Logistic Regression)** | 연속형 독립변수를 이용하여 이진 또는 다항 분류를 수행하는 통계적 모델 | 이메일이 스팸인지 아닌지 분류 |
| **의사결정나무 (Decision Tree), CART(Classification and Regression Tree), C5.0** | 트리 구조를 사용하여 데이터를 분류하는 기법, 직관적 해석이 가능 | 고객의 대출 승인 여부 결정 |
| **나이브 베이즈 분류 (Naive Bayes Classification)** | 확률 기반 분류 알고리즘으로, 조건부 확률을 이용해 예측 | 뉴스 기사 카테고리 분류 (정치, 경제 등) |
| **인공신경망 (Artificial Neural Network, ANN)** | 인간 뇌의 뉴런 구조를 모방한 딥러닝 기반의 분류 모델 | 손글씨 숫자 인식 (MNIST) |
| **서포트 벡터 머신 (Support Vector Machine, SVM)** | 초평면을 이용하여 최적의 분류 경계를 찾는 기법 | 얼굴 감지 시스템에서 얼굴 여부 판별 |
| **k-최근접 이웃 (k-Nearest Neighborhood, kNN)** | 새로운 데이터를 기존 데이터와 거리를 비교하여 분류 | 상품 추천 시스템에서 유사한 사용자 찾기 |
| **규칙기반 분류와 사례기반추론 (Case-Based Reasoning, CBR)** | 사전에 정의된 규칙을 사용하여 분류하는 방법 | 의료 진단 시스템에서 기존 환자 사례 비교 |

#### 2. 로지스틱 회귀분석

##### 1) 개요
- **범주형 종속변수를 예측하기 위한 회귀 모델**
- **오즈비(Odds Ratio) 기반 확률 예측**
- **회귀식:**
  $`
  \log \left( \frac{p(x)}{1 - p(x)} \right) = a + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_k X_k
  `$
- **R 코드 예제:**
  ```r
  glm(종속변수 ~ 독립변수1 + 독립변수2, family=binomial, data=데이터셋)
  ```

#### 3. 의사결정나무

##### 1) 정의와 특징
- **분류 및 예측을 위해 나무 구조를 사용하여 모델 생성**
- **쉽게 해석 가능하고 과적합 방지를 위한 가지치기(Pruning) 필요**

##### 2) 주요 활용
- **세분화(Segmentation):** 그룹별 특성 찾기
- **분류(Classification):** 데이터를 특정 그룹으로 분류
- **예측(Prediction):** 미래 값 예측
- **차원 축소 및 변수 선택:** 중요 변수를 선별하여 모델 성능 향상
- **교호작용 효과 파악:** 변수 간 상호작용을 분석하여 의사결정에 반영

##### 3) 장단점
| 장점 | 단점 |
|----------------------------------|----------------------------------|
| 해석이 용이하고 계산이 복잡하지 않음 | 과대적합 발생 가능성이 높음 |
| 대용량 데이터에서도 빠르게 적용 가능함 | 경계값 근처에서 오차가 큼 |
| 비정상 값과 불필요한 변수에 영향받지 않음 | 변수 중요도 판단이 어려움 |
| 수치형과 범주형 변수를 모두 활용 가능함 | - |
| 분류 정확도가 높음 | - |

##### 4) 분석 과정
1. **분석 분기 (O → 성공, X → 실패)**
2. **가지치기 (Pruning):** 필요 없는 가지 제거
3. **불순도 측정:** 지니계수, 엔트로피 지수 활용

##### 5) 불순도 측정 : 지니지수 (Gini Index)
$`
Gini(T) = 1 - \sum_{l=1}^{k} p_l^2
`$
- $` T `$: 특정 노드 또는 집합
- $` k `$: 클래스의 개수
- $` p_l `$: 클래스 $` l `$의 확률
- **지니지수가 낮을수록 데이터가 더 순수함**

###### 1. 첫 번째 집합 (불순도가 높은 데이터)
- 구성: ○ ● ▲ ● ○ ▲ ● ○ ● ▲ (총 10개)
- 각 기호 개수: ● 4개, ○ 3개, ▲ 3개
$` Gini = 1 - \left(\frac{4}{10}\right)^2 - \left(\frac{3}{10}\right)^2 - \left(\frac{3}{10}\right)^2 = 0.66 `$

###### 2. 두 번째 집합 (불순도가 낮은 데이터)
- 구성: ○ ● ● ● ● ● ● ○ ● ● (총 10개)
- 각 기호 개수: ● 8개, ○ 2개
$` Gini = 1 - \left(\frac{8}{10}\right)^2 - \left(\frac{2}{10}\right)^2 = 0.32 `$

##### 6) 불순도 측정 : 엔트로피 (Entropy)
$`
Entropy(T) = - \sum_{l=1}^{k} p_l \log_2 p_l
`$
- $` T `$: 특정 노드 또는 집합
- $` k `$: 클래스의 개수
- $` p_l `$: 클래스 $` l `$의 확률
- **엔트로피 값이 낮을수록 데이터가 더 순수함**

###### 1. 첫 번째 집합 (불순도가 높은 데이터)
- 구성: ○ ● ▲ ● ○ ▲ ● ○ ● ▲ (총 10개)
- 각 기호 개수: ● 4개, ○ 3개, ▲ 3개
$` Entropy = - \left(\frac{4}{10} \log_2 \frac{4}{10} + \frac{3}{10} \log_2 \frac{3}{10} + \frac{3}{10} \log_2 \frac{3}{10} \right) = 1.57 `$

###### 2. 두 번째 집합 (불순도가 낮은 데이터)
- 구성: ○ ● ● ● ● ● ● ○ ● ● (총 10개)
- 각 기호 개수: ● 8개, ○ 2개
$` Entropy = - \left(\frac{8}{10} \log_2 \frac{8}{10} + \frac{2}{10} \log_2 \frac{2}{10} \right) = 0.72 `$

#### 4. 나이브 베이즈 분류 (Naive Bayes Classification)

##### 1) 개념
- 데이터에서 변수들 간 **조건부 독립**을 가정하는 알고리즘
- 베이즈 정리(Bayes' theorem)를 이용하여 특정 클래스에 속할 확률을 계산
- 텍스트 분류(스팸 필터링, 감성 분석 등)에 주로 사용됨

##### 2) 베이즈 정리 (Bayes' Theorem)
베이즈 정리는 **사전 확률(Prior)과 사후 확률(Posterior) 사이의 관계**를 나타내는 수식임.

$`
P(A | B) = \frac{P(B | A) P(A)}{P(B)}
`$

- $`P(A|B)`$: 사건 B 발생 후 A가 발생할 확률 (사후확률, Posterior)
- $`P(B|A)`$: 사건 A 발생 후 B가 발생할 확률 (우도, Likelihood)
- $`P(A)`$: 사건 A의 발생 확률 (사전확률, Prior)
- $`P(B)`$: 사건 B의 발생 확률 (증거, Evidence)

##### 3) 분류 예제
한 대학에는 총 **1,000명의 학생**이 있으며, 이 중 **600명은 온라인 강의를 듣는다**.  
학생의 학년별로 보면 **학부생(Undergraduate)** 700명 중 **온라인 강의를 듣는 학생은 400명**,  
**대학원생(Graduate)** 300명 중 **온라인 강의를 듣는 학생은 200명**이다.  
이 대학에서 **임의로 한 명의 학생을 선택했을 때, 그 학생이 대학원생이라면 온라인 강의를 들을 확률**을 구하라.

###### 1. 확률 변수 정의
- **$`A`$**: 온라인 강의를 듣는 학생  
- **$`B`$**: 온라인 강의를 듣지 않는 학생  
- **$`C`$**: 대학원생

주어진 정보:
- $` P(A) = \frac{600}{1000} = 0.6 `$, $` P(B) = \frac{400}{1000} = 0.4 `$
- $` P(C | A) = \frac{200}{600} = \frac{1}{3} `$, $` P(C | B) = \frac{100}{400} = \frac{1}{4} `$

###### 2. 베이즈 정리 적용
**대학원생일 때 온라인 강의를 듣는 확률** $` P(A | C) `$ 계산:

$`
P(A | C) = \frac{P(A) P(C | A)}{P(A) P(C | A) + P(B) P(C | B)}
`$

$`
= \frac{0.6 \times \frac{1}{3}}{(0.6 \times \frac{1}{3}) + (0.4 \times \frac{1}{4})}
`$

$`
= \frac{0.2}{0.2 + 0.1} = \frac{0.2}{0.3} = 0.67
`$

#### 5. 인공신경망 (Artificial Neural Network, ANN)

##### 1) 개요
- **다층 퍼셉트론(MLP, Multi-layer Perceptron)** 기반의 모델로 **입력층, 은닉층, 출력층**으로 구성.
- **ReLU, Sigmoid, Tanh** 등의 활성화 함수를 사용하여 비선형성을 추가.
- **역전파(Backpropagation)와 경사 하강법(Gradient Descent)** 을 활용하여 학습 진행.

##### 2) 신경망 구조
| 구성 요소 | 설명 |
|------|------|
| **입력층** | 데이터를 입력받는 층. 특성 개수와 동일한 뉴런을 가짐. |
| **은닉층** | 비선형 변환을 수행하는 중간층. 층 수와 뉴런 수를 조정하여 최적화 가능. |
| **출력층** | 최종 예측을 수행하는 층. 회귀 문제는 1개 뉴런, 분류 문제는 클래스 개수만큼 뉴런을 가짐. |

##### 3) 학습 과정
- **순전파(Forward Propagation):** 입력 데이터가 가중치와 활성화 함수를 거쳐 출력을 생성.
- **역전파(Backpropagation):** 출력과 실제 값의 오차를 기반으로 가중치를 조정.
- **손실 함수(Loss Function):** 경사 하강법을 사용해 최소화.

##### 4) 신경망 모델 구축 시 고려사항
| 요소 | 설명 |
|------|------|
| **가중치 초기화** | 적절한 초기화 기법(He, Xavier) 사용. |
| **활성화 함수** | ReLU가 기울기 소실 문제를 완화, Sigmoid/Tanh는 깊은 신경망에서 기울기 소실 발생 가능. |
| **학습률** | 너무 크면 발산, 너무 작으면 학습 속도가 느림. Adam 등 적응형 학습률 활용. |
| **과적합 방지** | Dropout, L2 Regularization 등 사용. |

##### 5) 기울기 소실(Gradient Vanishing) 문제
- **문제:** 깊은 신경망에서 기울기가 작아져 학습이 제대로 이루어지지 않는 현상.
- **해결책:**  
  - ReLU 활성화 함수 사용  
  - Batch Normalization 적용  
  - Residual Connection 활용 (ResNet)

#### 6. 서포트 벡터 머신 (Support Vector Machine, SVM)

##### 1) 개요
- **지도학습(Supervised Learning) 기반의 분류(Classification) 및 회귀(Regression) 모델**.
- **결정 경계(Decision Boundary)를 최적화하여 데이터 분류**.

##### 2) SVM의 장점과 단점
| 항목 | 설명 |
|------|------|
| **장점** | 고차원에서도 효과적이며, 커널 함수를 활용하여 비선형 문제 해결 가능. |
| **단점** | 대규모 데이터셋에서는 학습 속도가 느릴 수 있으며, 커널 및 파라미터 선택이 중요. |

#### 7. k-최근접 이웃 (k-Nearest Neighbors, k-NN)

##### 1) 개요
- **지도학습(Supervised Learning) 알고리즘으로 분류(Classification) 및 회귀(Regression)에 사용**.
- **새로운 데이터 포인트를 가장 가까운 k개의 이웃과 비교하여 예측**.
- 모델 학습 과정이 없으며, **거리를 기반으로 즉시 분류를 수행하는 비모수적(Non-parametric) 방법**.

##### 2) k-NN의 구조
| 구성 요소 | 설명 |
|------|------|
| **k 값 (이웃 개수)** | 가장 가까운 k개의 데이터 포인트를 고려하여 분류 또는 예측. |
| **거리 측정 방법** | 유클리드 거리(Euclidean), 맨해튼 거리(Manhattan), 코사인 유사도(Cosine Similarity) 등 활용. |
| **가중치 부여 (Weighted k-NN)** | 가까운 이웃일수록 더 높은 가중치를 부여하여 예측의 신뢰도를 향상. |

##### 3) 학습 및 예측 과정
- **학습 과정 없음:** 데이터셋을 저장하고 새로운 데이터가 입력될 때 비교.
- **예측 과정:**  
  1. 새로운 데이터 포인트와 기존 데이터 사이의 거리를 계산.
  2. 가장 가까운 k개의 이웃을 찾음.
  3. 다수결 투표(Majority Voting) 또는 평균을 이용해 예측.

##### 4) k-NN 모델 구축 시 고려사항
| 요소 | 설명 |
|------|------|
| **k 값 선택** | 작은 k → 노이즈에 민감, 큰 k → 경계가 부드러워짐 (일반적으로 홀수 선택). |
| **거리 측정 방법** | 연속형 데이터는 유클리드 거리, 범주형 데이터는 해밍 거리(Hamming Distance) 활용. |
| **데이터 정규화** | 거리 기반 알고리즘이므로 표준화(Standardization) 또는 정규화(Normalization) 필요. |
| **고차원 문제** | 차원이 증가하면 거리의 의미가 희석되는 **차원의 저주(Curse of Dimensionality)** 발생 가능. |

##### 5) k-NN의 장점과 단점
| 항목 | 설명 |
|------|------|
| **장점** | 단순하고 직관적이며, 학습 과정 없이 즉시 적용 가능. |
| **단점** | 데이터가 많아질수록 계산량 증가, 고차원 데이터에서 성능 저하. |

#### 8. 앙상블 기법

##### 1) 개요
- **여러 개의 예측 모델을 조합하여 최적의 예측 모델 생성**
- **다중 모델 조합(Combining Multiple Models)**

##### 2) 주요 기법
| 기법 | 설명 |
|------|------|
| **배깅(Bagging, Bootstrap Aggregating)** | 여러 개의 **부트스트랩 샘플(랜덤 샘플링 후 복원추출)** 을 사용하여 학습한 모델들의 결과를 **평균**하여 최종 예측 수행 (예: 랜덤 포레스트) |
| **부스팅(Boosting)** | 이전 모델의 **오차에 가중치**를 부여하여 새로운 모델을 학습하는 방식으로, 약한 학습기(Weak Learner)를 순차적으로 학습하여 강력한 모델을 만듦 (예: **GBM, XGBoost, LightGBM**) |
| **랜덤 포레스트(Random Forest)** | 여러 개의 **의사결정나무(Decision Tree)** 를 조합하여 예측하며, 각 트리는 **무작위로 선택된 데이터와 특성(feature)을 사용**하여 학습 |

##### 3) 주요 부스팅 기법 비교

| 기법 | 특징 | 가중치 적용 방식 |
|------|------|----------------|
| **GBM (Gradient Boosting Machine)** | 잔차(residual)를 최소화하는 방식으로 **순차적 학습** | 학습 오류가 큰 샘플에 **더 높은 가중치** 부여 |
| **XGBoost (Extreme Gradient Boosting)** | **GBM을 개선**하여 속도와 성능 향상, 과적합 방지를 위한 **정규화(Regularization) 적용** | 가중치를 조정하면서 트리를 추가적으로 생성 |
| **LightGBM (Light Gradient Boosting Machine)** | **대용량 데이터에 최적화**, 리프(leaf)-wise 성장 방식으로 빠른 학습 속도 제공 | 손실 감소가 큰 노드부터 우선적으로 분할 |

### 제3절 군집분석(Clustering)

#### 1. 군집분석 개요

##### 1) 정의
- **유사한 개체들을 군집화하여 그룹을 나누는 기법**
- **비지도학습(Unsupervised Learning)** 의 대표적인 방법으로, 정답이 없는 데이터에서 패턴을 탐색하는 데 사용됨
- **데이터의 구조 파악 및 그룹 분류에 활용**

- **계층적 군집화 (Hierarchical Clustering)** : 데이터 간 **유사도를 기반으로 계층적 트리 구조(덴드로그램, Dendrogram)** 를 형성하는 방식. **병합(Agglomerative)** 과 **분할(Divisive)** 방식이 있음.
- **비계층적 군집화 (Non-Hierarchical Clustering)** : 데이터를 미리 정해진 개수의 군집으로 할당하는 방식. 대표적으로 **K-평균(K-Means)**, **DBSCAN**, **GMM(Gaussian Mixture Model)** 등이 있음.

##### 2) 특징
- **군집 내 개체 간 유사성이 높고, 군집 간 차이가 크도록 분류**
- **탐색적 데이터 분석에 효과적**
- **비지도 학습 기반으로 학습 데이터 없이 적용 가능**

#### 2. 거리 측정 방법
| 거리 측정 방법 | 설명 |
|------|------|
| **유클리드 거리 (Euclidean Distance)** | 두 점 사이의 **직선 거리**를 측정하는 가장 일반적인 방식. 연속형 데이터에서 자주 사용됨. |
| **맨해튼 거리 (Manhattan Distance)** | 두 점 사이의 거리를 **수직 및 수평 이동의 합**으로 계산. 격자형 데이터에서 적합. |
| **민코프스키 거리 (Minkowski Distance)** | 유클리드 거리와 맨해튼 거리를 일반화한 거리 측정 방식으로, **p 값 조정에 따라 다양한 거리 척도 적용 가능**. |
| **마할라노비스 거리 (Mahalanobis Distance)** | 변수 간 상관관계를 고려하여 거리 계산. **공분산 행렬을 이용**하여 특성이 다른 데이터에 적합. |
| **표준화 거리 (Standardized Distance)** | 데이터의 평균을 0, 분산을 1로 변환한 후 거리 계산. 데이터의 **스케일 차이를 보정**하는 데 유용. |
| **체비체프 거리 (Chebyshev Distance)** | 두 점 사이에서 **각 차원의 최댓값을 거리로 사용**. 체스판 이동 거리 계산에 응용됨. |
| **캠버라 거리 (Canberra Distance)** | 두 값의 차이를 두 값의 합으로 나눈 가중 거리 측정 방식. **값의 크기에 민감**하며, 비교적 작은 값에서도 차이를 강조할 때 사용. |
| **자카드 계수 (Jaccard Coefficient)** | 두 집합 간의 유사성을 측정하는 방법으로, **이진 변수(0,1) 데이터**에서 활용. |

#### 3. 계층적 군집분석 방법

| 방법 | 설명 |
|------|------|
| **최단연결법 (Single Linkage)** | 가장 가까운 두 개체 간 거리를 기준으로 군집 형성 |
| **최장연결법 (Complete Linkage)** | 가장 먼 두 개체 간 거리를 기준으로 군집 형성 |
| **평균연결법 (Average Linkage)** | 군집 내 모든 개체 간 평균 거리 사용 |
| **와드연결법 (Ward Linkage)** | 군집 내 분산을 고려하여 분류하는 방식 |

#### 4. 비계층적 군집분석

##### 1) K-평균 군집분석 (K-Means Clustering)
- **주어진 데이터를 K개의 군집으로 나누는 방법**
- **군집의 개수를 미리 정하고, 초기 군집 중심을 기반으로 재할당**
- **반복적으로 군집 중심을 조정하여 최적의 군집 형성**

###### 1. K-Means 과정
1. 초기 군집 중심(Seed) 설정
2. 각 데이터 포인트를 가장 가까운 군집 중심에 할당
3. 군집 내 데이터들의 평균을 계산하여 새로운 군집 중심으로 설정
4. 중심이 더 이상 변하지 않을 때까지 반복

###### 2. K-Means 장점과 단점

| 장점 | 단점 |
|------|------|
| 데이터가 클 때 빠르게 수행 가능 | 군집 개수를 미리 정해야 함 |
| 비교적 간단한 계산 과정 | 이상값에 민감함 |

#### 5. 혼합 분포 군집 (Mixture Distribution Clustering)

- **모델 기반 군집(Model-Based Clustering)** 기법
- **데이터가 특정 확률 분포를 따른다고 가정하고 군집을 형성**
- **EM(Expectation-Maximization) 알고리즘을 활용하여 최적 군집 탐색**

##### 1) 특징
- **확률적 모델을 기반으로 군집을 탐색**
- **K-Means보다 유연한 군집화 가능**
- **다양한 데이터 형태에서 적용 가능**

##### 2) 활용 사례

- **생물학적 데이터 분석**
- **유전자 발현 패턴 분석**
- **고객 세분화 분석**

#### 6. SOM(Self-Organizing Map, 자가조직화지도)

##### 1) 개요
- **코호넨 네트워크(Kohonen Network)** 기반의 **비지도 학습 신경망**.
- **고차원 데이터를 2D 또는 3D로 변환하여 시각적으로 표현**.
- **유사한 데이터가 가까운 위치에 배치되도록 학습**.

##### 2) 주요 개념
| 개념 | 설명 |
|------|------|
| **BMU(Best Matching Unit)** | 입력 벡터와 가장 유사한 뉴런. |
| **단 하나의 전방 패스** | 한 번의 순전파(Forward Pass)로 군집 형성. |
| **이웃 뉴런 업데이트** | BMU 주변 뉴런도 함께 조정. |
| **가중치 조정** | $`W_{new} = W_{old} + \alpha (X - W_{old})`$ |

##### 3) 학습 과정
1. **BMU 찾기:** 입력 벡터와 가장 가까운 뉴런 선택.
2. **BMU 및 이웃 뉴런 업데이트:** 점진적 조정으로 데이터 구조 유지.
3. **반복 학습:** 여러 번 반복하여 데이터 정렬.

##### 4) 특징 및 활용
| 항목 | 설명 |
|------|------|
| **특징** | 비지도 학습, 데이터 구조 보존, 시각적 표현 가능. |
| **활용** | 이미지 및 패턴 인식, 음성 신호 처리, 금융 데이터 분석. |

#### 7 주요 타당성 지표
| 지표 | 설명 |
|------|------|
| **실루엣 계수(Silhouette Coefficient)** | 군집 내 응집도(cohesion)와 군집 간 분리도(separation)를 비교하여 **1에 가까울수록 좋은 군집화**. |
| **Dunn 지수(Dunn Index)** | 군집 내 최장 거리와 군집 간 최소 거리의 비율을 계산, 값이 클수록 더 나은 군집화. |

### 제4절 연관분석(Association Analysis)

#### 1. 연관분석 개요

##### 1) 정의
- **거래 데이터에서 상품의 구매 패턴을 분석하여 규칙을 발견하는 방법**
- **대표적인 예: 장바구니 분석(Market Basket Analysis)**

##### 2) 활용 사례

- **상품 추천 시스템**
- **마케팅 전략 수립**
- **연관된 서비스 추천**

#### 2. 연관 규칙 형식

- **조건과 결과가 있는 If-Then 규칙**
  - **형식:**  $` \text{If (item set A) → (item set B)} `$
  - **예:** "우유를 구매한 고객은 빵을 구매할 가능성이 높다."

#### 3. 척도

| 척도 | 정의 | 수식 |
|------|------|------|
| **지지도 (Support)** | 전체 거래 중 특정 항목 집합이 동시에 포함되는 비율 | $` P(A \cap B) `$ |
| **신뢰도 (Confidence)** | 항목 A가 포함된 거래 중 항목 B가 포함될 확률 | $` \frac{지지도}{P(A)}  = \frac{P(A \cap B)}{P(A)} = P(B \| A) `$ |
| **향상도 (Lift)** | A와 B의 동시 발생 빈도가 독립적인 경우보다 얼마나 높은지 측정 | $` \frac{신뢰도}{P(B)} = \frac{P(A \cap B)}{P(A) P(B)} `$ |

#### 4. 연관분석 특징

##### 1) 절차
1. **최소 지지도 설정 (예: 5%)**
2. **최소 지지도 미만의 항목 제거**
3. **2개 항목 이상의 조합 생성**
4. **반복 수행하여 빈번한 규칙 탐색**

##### 2) 장단점

| 장점 | 단점 |
|------|------|
| 탐색적 분석 가능 | 계산량이 많아질 수 있음 |
| 비지도 학습 적용 가능 | 지지도 임계값 설정이 어려울 수 있음 |
| 데이터 사전 정제 없이 적용 가능 | 과적합 방지를 위한 적절한 규칙 필터링 필요 |

#### 5. 평가 기준 적용 시 주의점

- **신뢰도가 높아도 반드시 강한 연관성이 있는 것은 아님**  
- **지지도와 향상도를 함께 고려해야 함**  
- **전체 거래량이 적은 경우 신뢰도에 의한 왜곡 가능성 존재**  
- **A → B의 신뢰도가 높다고 해서 B → A의 신뢰도가 높다는 보장은 없음**

#### 6. Apriori 알고리즘

##### 1) 개요
- **최소 지지도(Support) 이상을 갖는 항목 집합을 빈발 항목 집합(Frequent Item Set)이라 정의**.
- 모든 항목 집합에 대한 지지도를 계산하는 것이 아니라, **최소 지지도 이상의 빈발 항목 집합만 선택하여 연관 규칙을 생성**.
- **Apriori 속성**: 빈발하지 않은 항목의 상위 집합도 빈발하지 않으므로, 불필요한 조합 생성을 줄일 수 있음.

##### 2) 예제
- **{우유, 빵, 과자}가 빈번하면, {우유, 빵}과 {우유, 과자}도 빈번함**
- **반복적으로 부분 집합을 탐색하여 연관 규칙 생성**

##### 3) 특징
| 항목 | 설명 |
|------|------|
| **장점** | 구현이 간단하고 이해하기 쉬운 1세대 알고리즘. |
| **단점** | 아이템 개수가 많아질수록 조합이 기하급수적으로 증가하여 계산량 부담 발생. |

## 제4장 비정형 데이터 마이닝

### 제1절 텍스트 마이닝

#### 1. 텍스트 마이닝(Text Mining)

##### 1) 개요
- **텍스트에서 의미 있는 정보를 추출하는 분석 방법**
- **비정형 데이터(문서, 기사, 리뷰 등)를 구조화하여 패턴을 도출**
- **자연어 처리 기법을 활용하여 감성 분석, 키워드 추출, 문서 요약 등에 활용**

##### 2) 활용 사례
- **감성 분석**
- **워드 클라우드**
- **문서 요약 및 분류**
- **특성 추출**

#### 2. Corpus (코퍼스)
- **데이터 마이닝에서 텍스트를 정제하고 변환하는 과정**
- **R의 'tm' 패키지를 활용하여 문서를 Corpus 형태로 변환**

| 함수 | 설명 |
|------|------|
| **VCorpus()** | 문서를 Corpus 객체로 변환 |
| **PCorpus()** | Corpus 객체를 R 내장 DB에서 관리 |

#### 3. R을 이용한 텍스트 마이닝
- **'tm' 패키지를 활용하여 문서 전처리 수행**

| 함수 | 설명 |
|------|------|
| **tm_map(data, as.PlainTextDocument)** | XML 문서를 text로 변환 |
| **tm_map(data, stripWhitespace)** | 공백 제거 |
| **tm_map(data, tolower)** | 소문자로 변환 |
| **tm_map(data, removeWords, stopwords("english"))** | 불용어(stopword) 제거 |
| **DocumentTermMatrix()** | 문서별 단어 빈도수 행렬 생성 |
| **TermDocumentMatrix()** | 단어별 문서 빈도수 행렬 생성 |

#### 4. 텍스트 전처리 (Text Preprocessing)

##### 1) 개요
- **텍스트 데이터를 모델이 처리할 수 있도록 변환하는 과정**.
- **정확한 분석을 위해 불필요한 요소를 제거하고 일관된 형태로 변환**.
- 일반적으로 **토큰화 → 불용어 처리 → 대소문자 통일 → 어근 추출 → 텍스트 인코딩** 순서로 진행.

##### 2) 텍스트 전처리 순서

| 단계 | 설명 | 예시 |
|------|------|------|
| **1. 토큰화 (Tokenization)** | 문장을 단어(Word) 또는 형태소 단위로 분리. | "I am learning NLP." → ["I", "am", "learning", "NLP"] |
| **2. 불용어 처리 (Stopword Removal)** | 의미 없는 단어 제거하여 분석 성능 향상. | ["I", "am", "learning", "NLP"] → ["learning", "NLP"] |
| **3. 대소문자 통일 (Lowercasing)** | 대소문자 구분이 필요 없는 경우 소문자로 변환. | ["Learning", "NLP"] → ["learning", "nlp"] |
| **4. 어근 추출 (Stemming, Lemmatization)** | 단어를 원형으로 변환하여 일관성 유지. | ["learning"] → ["learn"] |
| **5. 텍스트 인코딩 (Text Encoding)** | 모델이 이해할 수 있도록 수치 데이터로 변환. | ["learn", "nlp"] → [101, 345] (정수 인코딩) |

#### 5. 감성 분석

- **문장에서 사용된 단어를 분석하여 긍정/부정 여부 판단**
- **긍정 단어와 부정 단어의 비율을 비교하여 감정 점수 계산**

#### 6. 워드 클라우드

- **문서 내 단어의 빈도를 기반으로 시각화**
- **단어 크기와 색상으로 중요도를 표현**
- **R 패키지 'wordcloud' 활용**

### 제2절 사회연결망 분석

#### 1. 사회연결망 분석(Social Network Analysis)

##### 1) 개요
- **개인이나 집단 간 관계를 네트워크(graph)로 모델링하여 분석하는 기법**.
- **노드(Node)** 는 개별 객체(예: 사람, 조직)를, **링크(Link)** 는 이들 간의 관계(예: 소셜미디어 친구, 협업)를 의미.
- **연결 구조를 기반으로 네트워크의 위상(topology) 및 관계의 중요도를 평가**.

##### 2) 활용 사례
- **SNS 사용자 관계 분석**
- **기업 내 협업 네트워크 분석**
- **범죄 조직 및 금융 사기 네트워크 분석**

#### 2. 사회연결망의 분석 방식

| 분석 방법 | 설명 |
|------|------|
| **집합론적 방법 (Set Theory Approach)** | 관계를 개체의 집합으로 표현하며, **소속(Group Membership) 및 관계(Relationship)를 기반으로 분석**. |
| **그래프 이론 기반 방법 (Graph Theory Approach)** | 네트워크를 **노드(Node)와 엣지(Edge)로 모델링**하여 연결 구조를 분석. 중심성(Centrality), 커뮤니티 탐색(Community Detection) 등의 기법 활용. |
| **행렬 기반 방법 (Matrix Approach)** | 네트워크를 **인접행렬(Adjacency Matrix) 또는 발생행렬(Incidence Matrix)로 변환**하여 수학적 연산을 통해 관계 강도 및 구조 분석. |

#### 3. 네트워크 구조 분석 기법

| 기법 | 설명 |
|------|------|
| **연결정도 중심성 (Degree Centrality)** | 한 개체에 연결된 직접적인 관계 수 |
| **근접 중심성 (Closeness Centrality)** | 네트워크 내 개체 간 최단 경로 분석 |
| **매개 중심성 (Betweenness Centrality)** | 네트워크 내 개체가 다른 개체 간 경로에서 중간 역할 수행 정도 |
| **위세 중심성 (Eigenvector Centrality)** | 영향력이 높은 개체와 연결된 정도 |

#### 3. 사회연결망의 중심성 분석 기법

##### 1) 개요
사회연결망 분석(SNA)에서 **중심성(Centrality)은 네트워크 내에서 특정 노드(Node)가 얼마나 중요한 역할을 하는지 측정하는 지표**.  
주로 **연결 정도, 근접성, 매개성, 위세 중심성** 등의 방법이 사용됨.

##### 2) 주요 중심성 지표
| 기법 | 설명 | 예시 |
|------|------|------|
| **연결정도 중심성 (Degree Centrality)** | 직접 연결된 관계(링크) 수가 많을수록 중요도가 높음. | **SNS에서 친구 수가 많은 사용자** |
| **근접 중심성 (Closeness Centrality)** | 네트워크 내 다른 노드들과 평균 거리가 짧을수록 중요함. | **회사 내 가장 빠르게 소식을 전달받는 직원** |
| **매개 중심성 (Betweenness Centrality)** | 정보 흐름을 중개하는 역할이 많을수록 중요함. | **조직 내 부서 간 커뮤니케이션 담당자** |
| **위세 중심성 (Eigenvector Centrality)** | 영향력 있는 노드와 연결될수록 중요도가 증가. | **SNS에서 유명 인플루언서와 연결된 사용자** |

# 과목5. 데이터 시각화

## 제1장 시각화 인사이트 프로세스

### 제1절 시각화 인사이트 프로세스의 의미

#### 1. 인사이트란 무엇인가

##### 1) 인사이트의 사전적 정의
- 통찰. 예리한 관찰력으로 사물을 꿰뚫어 봄
- 새로운 사태에 직면하여 장면의 의미를 재조직함으로써 문제를 해결하는 것 또는 그런 과정

##### 2) 데이터, 정보, 지식, 지혜의 관계
- DIKW 피라미드는 데이터(Data), 정보(Information), 지식(Knowledge), 지혜(Wisdom) 사이의 계층적 관계를 시각적으로 표현한 것
- 지식의 단계에서는 인류가 그동안 축적한 축적된 정보가 조직적으로 재구성되며 새로운 의미가 도출됨
- 지혜는 개인화된 지식으로 볼 수 있음

| 단계 | 시각 위계 | 설명 | 예시 |
|------|------------------|------|------|
| **데이터 (Data)** | **시각화 (Visualization)** | 개별적인 원시 데이터로 의미 없이 존재 | "사용자의 웹사이트 방문 로그" |
| **정보 (Information)** | **디자인 (Design)** | 데이터 간의 관계를 분석하여 의미를 생성 | "사용자별 페이지 방문 빈도 분석" |
| **지식 (Knowledge)** | **매핑 (Mapping)** | 조직화된 정보로 패턴과 상관관계를 도출 | "방문 패턴에 따른 추천 시스템 구축" |
| **지혜 (Wisdom)** |  | 지식을 바탕으로 최적의 의사결정을 수행 | "개인 맞춤형 광고 노출로 전환율 증가" |

#### 2. 시각화와 인사이트

##### 1) 삼찰: 관찰, 성찰, 통찰
- **관찰** : 외부 세계의 온갖 대상과 그 대상들이 상호작용을 관찰하면서 의미있는 관계를 찾아내는 것
- **성찰** : 자신이 내면 세계를 깊이 살펴보는 것
- **통찰** : 관찰과 성찰을 기반으로, 내부와 외부 요인들 간의 관계를 통하여 해법을 찾는 것

##### 2) 시각화와 인사이트
- 시각화를 활용하면 대상과 대상들 사이의 숨겨진 관계를 찾아냄으로써 인사이트를 얻을 수 있음

### 제2절 탐색(1단계)

#### 1. 사용 가능한 데이터 확인
- **데이터 명세화: 차원과 측정값**
  - 모든 데이터는 기본적으로 하나 이상의 측정값과 차원을 포함.
  - 값이 같거나 측정된 기준을 의미하며, 이를 통해 데이터를 분류 가능.
  - 예: 온라인 쇼핑몰의 주문 데이터(상품명, 구매수량, 결제금액 등).

- **데이터 구성 원리 1: 이벤트 기록으로서의 접근**
  - 원 데이터(Raw Data, Log Data)는 특정 이벤트가 발생했을 때 생성됨.
  - 예: 웹사이트 방문 로그(방문 시간, 클릭한 페이지, 체류 시간 등).

- **데이터 구성 원리 2: 객체지향 관점에서의 접근**
  - 데이터 구조의 생성을 설계하고 생명 주기를 고려하여 분석.
  - 객체지향 관점(Object Oriented) 적용: 데이터의 행위와 속성값을 분리하여 구조화.
  - 예: 고객 객체(이름, 나이)와 행동 객체(구매내역, 접속로그) 분리.

#### 2. 연결 고리의 확인
- **공통 요소 찾기**
  - 서로 다른 데이터의 명세에서 공통 항목 식별.
  - 동일한 규칙이 적용되는 데이터 항목은 명확하게 공통 요소로 활용.
  - 예: 고객 ID를 기준으로 구매 데이터와 상담 데이터 연결.

- **공통 요소로 변환하기**
  - 다르더라도 변환을 통해 공통 요소를 만들 수 있음.
  - 계층 관계나 스프레드시트의 `lookup`, `vlookup` 등을 활용한 변환 가능.
  - `지오코딩(Geocoding)`: 좌표(주소)를 통한 행정구역 변환.
  - 예: IP 주소를 기반으로 사용자 지역(도시, 국가) 변환.

#### 3. 관계의 탐색
- **이상값 처리**
  - 이상값(Outlier): 다른 관측값과 동떨어진 값.
  - 발생 이유: 
    - 측정 과정의 오류.
    - 데이터 기록·관리 과정에서의 문제 발생.
    - 원가 의미값의 영향.
  - 예: 평균보다 10배 이상 높은 결제 금액 감지 및 검토.

- **차원과 측정값 유형에 따른 관계 시각화**
  - 패턴 변화 탐색이 중요.
  - **모션차트(Motion Chart)**: 시간에 따른 데이터 변화를 시각적으로 표현.
  - **텍스트 데이터 시각화**: 비정형 데이터의 관계 탐색.
  - **워들(Wordle)**: NLP 기법을 활용하여 단어 빈도와 크기를 기반으로 분석.
  - 예: 연도별 인기 검색어 변화를 워드 클라우드로 시각화.

#### 4. 데이터 탐색 기법
- **잘라보고 달리보기(Slicing & Dicing)**
  - 데이터를 여러 기준으로 나누어 세부적으로 분석하는 기법.
  - 예: 지역별, 연령대별 매출 데이터를 각각 비교.

- **내려다보고 올려다보기(Drill Down & Roll Up)**
  - 데이터를 세분화(Drill Down)하거나 요약(Roll Up)하여 분석하는 방법.
  - 예: 월별 매출 데이터를 주별, 일별로 세분화(Drill Down)하거나 연도별로 집계(Roll Up).

- **척도의 조정(Scaling)**
  - 데이터 간 비교를 용이하게 하기 위해 값의 크기를 변환하는 과정.
  - 예: 제품 리뷰 점수를 1~100점에서 1~5점 척도로 변환하여 표준화.

### 제3절 분석(2단계)

#### 1. 분석 목표와 분석 기법

| 분석 기법 | 분석 목표 | 예제 |
|-----------|---------------------------------|--------------------------------------|
| **T검정** | 평균에 대한 추정과 검정 | 두 그룹(남성과 여성)의 평균 키 차이 검정 |
| **상관분석** | 변수들 간의 상관관계 강도 도출 | 광고비와 매출 간의 상관관계 분석 |
| **카이제곱검정, 잔차분석, Fisher, 맥네마** | 분할표의 검정 | 고객 성별과 구매 유형 간의 독립성 검정 |
| **요인분석, 주성분분석** | 변수들 간 관계 및 핵심 요인 선별 | 설문조사 문항을 요인으로 축소하여 분석 |
| **직접확률계산법, F분포법** | 비율에 대한 추정과 검정 | 특정 제품의 시장 점유율 비교 검정 |
| **시계열분석** | 시간의 흐름에 따른 데이터 분석 | 주식 시장의 가격 변동 예측 |
| **선형/다중/로지스틱 회귀분석, 판별분석** | 변수들 간 인과관계 형태와 강도 추출 | 고객 소득과 소비 패턴 간의 관계 분석 |
| **군집분석, 다차원척도법** | 다차원 기준에 따른 대상 분류 | 고객 구매 성향을 기반으로 군집화 |
| **대응분석** | 차원 패턴을 기반으로 데이터 분류 | 브랜드별 고객 선호도 분석 |

### 제4절 활용(3단계)

#### 1. 내부에서 적용
- **통찰 활용**: 
  - 여러 형태로 응용 가능하며, 기존 통찰을 보완하거나 새로운 통찰을 발견 가능
  - 내부 적용 방식:
    1. 기존 문제 해결 방식·설명 모델 수정
    2. 새로운 문제 해결 방식 도입
    3. 새로운 가능성에 대한 구체적인 탐색

- **적용 예시**:
  1. 기존 설명력을 확신 강화하는 변인 추가/상수값 보정
  2. 서비스 개선 요소 및 신규 모델 발굴
  3. 조직 정보 체계 구축

#### 2. 외부에 대한 설명·설득과 시각화 도구
- **설명과 설득의 필요성**:
  - 통찰을 이해관계자에게 설명하고 설득하는 과정 필요
  - 그림이나 그래프 등 시각적 도구를 활용해 효과적으로 전달

- **설득을 위한 요소**:
  - 상대방이 공감하고 행동하도록 하는 강력한 상호작용 필요
  - 감성적인 측면도 고려해야 하며, 디자인(Design)의 역할이 중요

#### 3. 인사이트의 발전과 확장

##### 1) 탑다운 vs 보텀업
- **탑다운 방식**: 새로운 대상 탐색 시 적절.  
  - 예: 기업이 신사업 기획 시 시장조사 데이터를 기반으로 유망 산업을 선정.
- **보텀업 방식**: 전제 없이 밑바닥에서부터 다양한 가능성을 찾는 방식.  
  - 예: 고객 리뷰 데이터를 분석하여 예상치 못한 니즈를 발견하고 신규 제품 개발.

##### 2) 2차 잘라보기·달리보기·내려다보기·올려다보기
- **중심적 시각 체크**: 기존 통찰에서 도출한 데이터 현실성과 분석 모델의 적정성 확인.  
  - 예: 마케팅 캠페인 결과 분석 시, 연령대별 성과 차이를 고려하여 캠페인 효과 재평가.

##### 3) 실시간 vs 비실시간
- **실시간 분석의 어려움**:
  - 데이터 양이 많을 경우 실시간 처리 어려움.
  - 탐색과 분석 수준이 낮아질 가능성 있음.  
  - 예: 주식 시장에서 거래 데이터를 실시간 분석하여 투자 판단.
- **비실시간의 필요성**:
  - 주기적인 샘플 기반 분석 활용 가능.
  - 즉각적인 실시간 처리가 필요하지 않은 경우 비실시간 활용.  
  - 예: 연말 매출 데이터를 집계하여 연간 실적 보고서 작성.

##### 4) 지표에서의 운영
- **지표 변화 요인 파악**:
  - 단순한 평균보다는 변화 요인과 흐름을 분석하는 것이 중요.  
  - 예: 웹사이트 방문자 수 증가가 광고 효과인지 계절적 요인인지 분석.

##### 5) 시각화의 오류
- **시각적 도구의 한계**:
  - 분석 과정에서 오류 발생 가능성 있음.
  - 다양한 관점에서 신중하게 고려 필요.  
  - 예: 축 범위를 조정하여 데이터 변동이 과장된 그래프가 의사결정에 영향을 미칠 수 있음.

##### 6) 사람의 문제
- **기계가 아닌 사람의 역할**:
  - 인사이트 도출 과정에서 사람의 판단이 중요한 역할 수행.
  - 개인차에 따라 통찰이 다르게 발생할 수 있음.  
  - 예: 같은 데이터를 분석해도 경영진과 현장 실무자의 해석이 다를 수 있음.

## 제2장 시각화 디자인

### 제1절 시각화의 정의

#### 1. 데이터 시각화의 중요성
- **데이터 시각화(Data Visualization)의 의미**
  - 방대한 양의 데이터를 분석하고 한눈에 볼 수 있도록 도표나 차트로 정리하는 과정
  - 데이터 분석과 의사소통이라는 두 가지 목적을 가짐

#### 2. 시각 이해와 시각화
- **시각 이해의 위계 (Hierarchy Of Visual Understanding)**

| 단계 | 설명 | 시각적 요소 |
|------|----------------------------------|----------------|
| **지혜(Wisdom)** | 추상적 개념 (철학, 사상, 법칙 등) | |
| **지식(Knowledge)** | 개념의 구성, 복잡한 구상 (이론, 논리 등) | **매핑(Mapping)**  |
| **정보(Information)** | 데이터 가공을 통한 해석 (패턴, 관계 도출) | **디자인(Design)** |
| **데이터(Data)** | 원자재 같은 연구 자료 (숫자, 단어, 로그 등) | **시각화(Visualization)** |

#### 3. 시각화 분류와 구분

| 시각화 유형 | 설명 | 예시 |
|-------------|---------------------------------|----------------------------------|
| **데이터 시각화** | 그래픽 요소를 이용한 명확한 커뮤니케이션 | 막대그래프, 파이차트, 산점도 |
| **정보 시각화** | 대규모 비정형 데이터를 시각적으로 표현 | 네트워크 그래프, 히트맵, 워드클라우드 |
| **정보 디자인** | 사용자가 쉽게 이해할 수 있도록 복잡한 정보를 구조화 | 웹사이트 대시보드, UI/UX 설계 |
| **인포그래픽** | 정보를 시각적으로 쉽게 전달하는 그래픽 메시지 활용 | 신문·잡지의 인포그래픽 기사, 포스터 |

### 제2절 시각화 프로세스

#### 1. 정보 디자인 프로세스

| 단계 | 설명 | 예시 |
|------|------------------------------------------------|--------------------------------------------|
| **1. 데이터 수집** | 분석을 위한 원시 데이터를 확보하는 과정. | 웹사이트 트래픽 로그, 설문조사 결과 수집. |
| **2. 모든 것을 읽기** | 데이터를 탐색하고 내용과 특성을 파악. | 누락된 값, 이상치, 데이터 구조 확인. |
| **3. 패턴 찾기** | 데이터 내 의미 있는 패턴과 관계를 식별. | 특정 시간대에 방문자 증가 패턴 확인. |
| **4. 문제의 정의** | 분석의 목적과 해결해야 할 핵심 문제를 설정. | "방문자 수 증가 요인을 분석하자." |
| **5. 계층 구조 만들기** | 데이터 요소 간의 관계를 구조화. | 방문자 → 유입 채널 → 행동 패턴으로 계층화. |
| **6. 와이어프레임 그리기** | 데이터 시각화의 초기 레이아웃을 설계. | 대시보드 레이아웃 초안 작성. |
| **7. 포맷 선택하기** | 적절한 차트, 그래프, 인포그래픽 유형 결정. | 시간별 트렌드 분석을 위해 선 그래프 선택. |
| **8. 시각 접근 방법 결정하기** | 색상, 크기, 레이아웃 등 디자인 요소 확정. | 방문자 수 변화 강조를 위해 색상 변화 적용. |
| **9. 정보 해석** | 시각화된 데이터를 통해 의미 있는 인사이트 도출. | SNS 유입 증가가 방문자 증가의 주요 원인. |
| **10. 세상에 선보이기** | 최종 결과물을 공유하고 적용. | 마케팅 팀과 공유하여 SNS 광고 전략 수립. |

#### 2. 빅데이터 시각화 프로세스

| 정보형 메시지 (벤프라이 7단계) | 정보 디자인 교과서 | 설득형 메시지 (마티아스 샤피로 3단계) |
|---------------------------|------------------|----------------------------------|
| 정보획득 | 범위 파악 / 목적 설정 | 질문을 만들어내기 |
| 분석(=분해) | 데이터 수집 / 마이닝 | 데이터를 수집하기 |
| 선별 | - | - |
| 마이닝 | - | - |
| 표현 | 그래프 가공 | - |
| 개선(=정제) | 시각적 표현 | 시각적 표현하기 |
| 상호작용 | - | - |

#### 3. 빅데이터 시각화 단계별 설명
| 단계 | 설명 |
|------|--------------------------------|
| **1단계: 정보 구조화** | 데이터 수집 및 정제, 데이터세트 구성 |
| **2단계: 정보 시각화** | 분석 도구 활용한 그래픽화 |
| **3단계: 정보 시각적 표현** | 의도를 강화하기 위해 그래픽 요소 추가 |

### 제3절 시각화 방법

#### 1. 정보 구조화
##### 1) **정보의 조직화 과정**
1. **데이터 수집 및 탐색**
   - 원본 데이터를 정리하고 필요한 자료를 수집.
   - 예: 다양한 형태(○ ▲ ■)의 데이터를 모음.

2. **데이터 분류**
   - 데이터를 유형별로 정리하여 유사한 기준에 따라 묶음.
   - 예: 동그라미(○), 세모(▲), 네모(■)로 그룹화.

3. **데이터 배열**
   - 데이터를 유의미한 구조로 정리하여 패턴을 형성.
   - 예: 크기 순서대로 정렬 (◯ ◑ ●).

4. **관계 맞추기**
   - 데이터 간 연관성을 설정하여 조직화.
   - 예: 서로 연결된 데이터끼리 관계를 정의 (○→▲→■).

##### 2) **데이터 분류 방식**
- **구분 텍스트**: 공백, 세미콜론, 콜론 등으로 구분
- **JSON**: 웹 클라이언트와 서버 간 데이터 전송 포맷
- **XML**: 확장 마크업 언어로 데이터 저장 및 전송에 활용

##### 3) **데이터 배열 (LATCH 방법)**
  - 위치(Location), 알파벳(Alphabet), 시간(Time), 카테고리(Category), 위계(Hierarchy)

#### 3. 정보 시각화

| 유형 | 설명 | 예시 |
|------|---------------------------------|------------------------------------------|
| **시간 시각화** | 시간에 따른 데이터 변화 추이를 표현 | 막대그래프, 누적 막대그래프, 점그래프 |
| **분포 시각화** | 데이터의 빈도와 분포를 확인 | 파이차트, 도넛차트, 트리맵, 누적 연속그래프 |
| **관계 시각화** | 변수 간 상관관계를 분석 | 스캐터 플롯, 버블차트, 히스토그램 |
| **비교 시각화** | 여러 요소 간 차이를 비교 | 히트맵, 체르노프 페이스, 스타차트, 다차원 척도법, 평행좌표계 |
| **공간 시각화** | 지리적 정보를 지도 기반으로 표현 | 지도 매핑 |

| 그래프 유형 | 특징 | 예시 |
|------------|------------------------------------------------|----------------------------------|
| **막대그래프** | 범주형 데이터를 막대의 길이로 비교하며, 각 항목 간 차이를 강조 | 연도별 매출 비교, 부서별 성과 분석 |
| **누적 막대그래프** | 여러 범주를 누적하여 개별 요소와 전체 변화를 동시에 표현 | 부서별 프로젝트 기여도, 제품별 시장 점유율 |
| **점그래프** | 개별 데이터 포인트를 점으로 나타내며, 데이터의 분포와 밀도를 쉽게 파악 | 특정 제품의 월별 판매량, 실험 데이터 시각화 |
| **파이차트** | 전체 대비 각 항목의 비율을 원형으로 나타내어 비율 비교에 효과적 | 시장 점유율 분석, 예산 항목별 비율 비교 |
| **도넛차트** | 파이차트와 유사하나 중심이 비어 있어 다중 데이터 비교 가능 | 예산 분포 분석, 마케팅 채널별 기여도 |
| **트리맵** | 계층적 데이터를 사각형 면적 비율로 표현하여 상대적 크기 비교 | 웹사이트 방문 경로 분석, 기업별 매출 비중 |
| **누적 연속그래프** | 시계열 데이터의 변화를 누적하여 표현하며, 시간에 따른 총합의 변화를 강조 | 월별 누적 사용자 증가 추이, 연간 총 매출 변화 |
| **스캐터 플롯** | 두 변수 간 관계를 점으로 시각화하여 상관관계를 분석 | 광고비와 매출 간의 관계 분석, 키와 몸무게 관계 연구 |
| **버블차트** | 스캐터 플롯에 크기(제3의 변수)를 추가하여 정보를 증가 | 기업별 매출과 시장 점유율 비교, 국가별 GDP와 인구수 분석 |
| **히스토그램** | 연속형 데이터의 빈도를 막대로 표현하여 데이터의 분포를 파악 | 시험 점수 분포 분석, 연령대별 고객 수 분석 |
| **히트맵** | 색상의 강도를 이용하여 값의 크기와 빈도를 직관적으로 표현 | 웹사이트 클릭 데이터 분석, 날씨 변화 패턴 시각화 |
| **체르노프 페이스** | 다변량 데이터를 얼굴 형태로 변환하여 비교하며, 여러 변수를 동시에 시각화 | 고객 만족도 평가, 다양한 요인에 따른 제품 평가 |
| **스타차트** | 다변량 데이터를 방사형으로 표현하여 여러 항목을 동시에 비교 | 스포츠 선수의 능력 비교, 제품 특성 비교 |
| **다차원 척도법** | 고차원 데이터를 2D 또는 3D 공간으로 축소하여 데이터 간의 유사도 표현 | 설문 응답자의 그룹 분석, 브랜드 이미지 비교 |
| **평행좌표계** | 다변량 데이터를 선형으로 연결하여 변수 간의 관계를 표현하며 상관관계 분석에 유리 | 다양한 요인의 관계 분석, 금융 상품 리스크 평가 |
| **지도 매핑** | 지리적 데이터를 지도 위에 시각적으로 표현하여 지역별 패턴 분석에 효과적 | 국가별 인구 밀도 분석, 감염병 확산 경로 시각화 |

#### 4. 정보 시각표현
- **그래픽 요소 7원칙**
  - 위치(Position), 크기(Size), 모양(Shape), 색(Color), 명도(Value), 기울기(Orientation), 질감(Texture)

- **그래픽 디자인 기본 원리**
  - **타이포그래피**: 서체, 크기, 두께, 스타일 적용
  - **색상**: 구분 표현, 순서 표현, 비율 표현
  - **그리드**: 정렬, 패턴 활용, 가이드라인 설정
  - **아이소타이프**: 숫자를 도형화하여 표현

#### 5. 인터랙션
- **비선형적 구조**: 사용자 임의 접근 가능
- **강조 & 디테일**: 정보의 여러 측면을 시각적으로 표현
- **사용자 조정 가능성**: 데이터 변환, 시각 맵핑 변경 지원
- **사용자 반응 반영**: 시각적 피드백 제공

#### 6. 시각 정보 디자인 7원칙

| 원칙 | 설명 | 예시 |
|------|----------------------------------|------------------------------------------|
| **1. 시각적 비교 강화** | 트렌드와 변화를 비교할 수 있는 도구 제공 | 연도별 매출 변화를 막대그래프로 표현 |
| **2. 인과관계 제시** | 정보 디자인 시 원인과 결과를 명확하게 표현 | 기온 상승과 전력 사용량 변화를 선 그래프로 연결 |
| **3. 다중 변수 표시** | 여러 개의 연관 변수를 활용하여 정보 표현 | 매출, 광고비, 고객 수를 한 그래프에서 비교 |
| **4. 텍스트, 그래픽, 데이터 조화 배치** | 라벨과 범례를 활용한 다이어그램 구성 | 제품별 매출과 고객 리뷰를 결합한 시각화 |
| **5. 콘텐츠의 질과 연관성, 진실성 확보** | 정보가 사용자 목표 달성에 유용한지 고려 | 설문조사 결과를 왜곡 없이 그래프로 표현 |
| **6. 시간순이 아닌 공간순 나열** | 시간보다 공간 배치가 더 직관적일 때 활용 | 도시별 인구밀도를 지도 기반으로 시각화 |
| **7. 정량적 자료의 정량성 유지** | 트렌드 표현 시 숫자를 유지하여 전달력 강화 | 도표와 그래프를 함께 사용하여 데이터 왜곡 방지 |

### 제4절 빅데이터와 시각화 디자인

#### 1. 빅데이터와 시각화 디자인의 방향
- **정보 분석 및 표현 역량 필수**: 최신 기술과 도구 활용 필요.  
- **심리적 반응 고려**: 단순한 그래픽이 아닌 인식 과정 반영.  
- **명확한 정보 디자인 목표**: 시각적 목표 설정 및 일관성 유지.  
- **데이터와 그래픽의 유기적 연결**: 수집, 가공, 표현 과정이 통합되어야 함.  
- **정보성 유지 필수**: 정보성이 부족하면 효율성과 참신성도 감소할 수 있음.

## 제3장 시각화 구현

### 제1절 시각화 구현 개요

#### 1. 빅데이터 시각화 구현 개요
- **비즈니스 인텔리전스(BI) 활용**: 데이터 기반 의사결정 지원.  
- **지식 시각화 기능 제공**: 다양한 관점에서 인사이트 도출 가능.  
- **오픈 소스 및 라이브러리 기반**: 프로젝트 형태 또는 라이브러리로 배포됨.

#### 2. 대표적인 시각화 방법

| **분류** | **도구 리스트** | **적용 방법** |
|------------|-------------------------------|-----------------------------|
| **시각화 플랫폼** | Cognos Insight, PowerPivot, QlikView, Tableau, SAS Enterprise BI, Gephi 등 | **플랫폼 설치 필요**, 제공되는 기능을 활용한 명령어 기반 시각화 |
| **시각화 라이브러리** | Flot, D3.js, Highcharts, Google Charts, Leaflet, NodeBox 등 | **라이브러리 설치 필요**, API 코드 작성 후 시각화 |
| **인포그래픽 도구** | iCharts, Visualize Free, Visual.ly 등 | **웹서비스 기반**, 템플릿을 활용한 시각화 |

### 제2절 분석 도구를 이용한 시각화 구현

#### 1. 시각화 구현 (R)
- `ggplot2` 패키지를 사용하여 다양한 그래프 생성

##### 1) 주요 그래프 유형

| 함수 | 그래프 유형 | 특징 | 사용 예시 |
|------|------------|--------------------------------|--------------------------|
| **`geom_point()`** | 산점도 (점 그래프) | 개별 데이터 포인트를 점으로 표현하여 변수 간 관계 시각화 | 변수 간 상관관계 분석, 데이터 분포 확인 |
| **`geom_line()`** | 선 그래프 | 연속적인 데이터를 선으로 연결하여 시간 변화나 추세 강조 | 주가 변동, 기온 변화, 시간 흐름에 따른 매출 분석 |
| **`geom_bar()`** | 막대 그래프 | 범주형 데이터를 막대의 길이로 표현하여 비교 | 제품별 매출 비교, 설문 응답 비율 |
| **`geom_histogram()`** | 히스토그램 | 연속형 데이터의 빈도를 막대 그래프로 표현하여 분포 분석 | 시험 점수 분포, 고객 연령대 분석 |
| **`geom_boxplot()`** | 박스플롯 | 중앙값, 사분위수, 이상치를 포함하여 데이터 분포를 요약 | 급여 분포 비교, 실험 결과의 변동성 분석 |
| **`geom_smooth()`** | 부드러운 추세선 | 데이터 패턴을 부드럽게 연결하여 전체적인 흐름 시각화 | 판매량 추세 분석, 기온 변화 경향 |
| **`geom_text()`** | 텍스트 추가 | 데이터 포인트에 값을 추가하여 정보 전달력 강화 | 특정 값 강조, 그래프 내 주요 지점 라벨링 |
| **`geom_density()`** | 밀도 그래프 | 데이터의 분포를 곡선 형태로 표현하여 패턴 분석 | 인구 밀도 분석, 제품 가격 분포 시각화 |

#### 2. 시각화 구현 (샤이니)
##### 1) 개요
- **R 기반 대화형 웹 애플리케이션 프레임워크**
- **데이터 시각화 및 분석 기능 제공**
- **UI와 서버 로직을 기반으로 반응형 애플리케이션 구축 가능**

##### 2) 주요 기능
- **인터랙티브 데이터 시각화**: ggplot2, plotly 등과 연계 가능  
- **동적 대시보드 개발**: 사용자 입력값에 따른 실시간 분석 가능  
- **다양한 UI 컴포넌트 지원**: 슬라이더, 드롭다운, 버튼 등 제공  
- **클라이언트-서버 구조**: UI(`ui.R`)와 서버 로직(`server.R`) 분리

##### 3) Shiny 구조
- **`ui.R` (사용자 인터페이스)**: 입력 요소 및 데이터 출력 화면 구성  
- **`server.R` (서버 로직)**: 데이터 처리 및 반응형 연산 수행

##### 4) 장점
- **코드만으로 웹 애플리케이션 구축 가능**
- **대화형 시각화 및 실시간 분석 지원**
- **웹 배포 및 공유 용이**

##### 5) 단점
- **대용량 데이터 처리 성능 제한**
- **프런트엔드 디자인 커스터마이징 어려움**
- **웹 서버 배포 환경 구축 필요**

### 제3절 라이브러리 기반의 시각화 구현: D3.js

#### 1. D3.js 개요
- **오픈 소스 프로젝트**로 무료 사용 가능
- 크롬, 파이어폭스, 사파리 등 다양한 웹 브라우저에서 **동일한 결과 보장**
- 주로 HTML 문서의 **SVG 객체**를 활용하여 시각화 구현(일부 canvas)
- CSS를 통해 객체의 **레이아웃과 속성을 조작 가능**
- 데이터 표현 단계에서 가장 중요한 요소는 **매핑과 스케일**

##### 1) **D3.js 시각화 구현 절차**
1. **데이터 획득**  
2. **데이터 파싱**  
3. **데이터 필터링**  
4. **데이터 표현**  
5. **상호작용 추가**

#### 2. 막대 차트로 시간 시각화 구현
- **`scale` 활용**: 데이터 값을 직접 지정하지 않고, **각 데이터에 맞는 크기와 컬러 범위**를 자동으로 설정하여 최적화
- **도메인 범위(domain range) 설정**이 중요하며, 이를 통해 축과 값의 관계를 조정할 수 있음

**좌표 및 회전 설정**
- `translate(a, b)`: 시각화 요소의 위치를 변경  
  - `a`: x축 이동  
  - `b`: y축 이동  
- `rotate(c)`: 요소를 `c`도만큼 회전

#### 3. 스캐터 플롯으로 관계 시각화 구현
스캐터 플롯은 데이터 간 **상관관계를 분석**하는 데 활용된다.  
D3.js에서는 다음과 같은 코드로 구현할 수 있다:

```js
d3.select("body").append("svg") // SVG 영역 생성
d3.scale.linear() // 선형 스케일 생성
d3.svg.axis() // 축 설정
```
각 데이터에 대한 포인트를 `circle` 객체로 생성하여 표현할 수 있다.

#### 4. 히트맵으로 비교 시각화 구현
히트맵을 구현하려면 **canvas 객체**가 필요하다.  
SVG와 Canvas는 각각의 특성에 따라 적절히 선택해야 한다.

| 항목 | SVG | Canvas |
|------|----------------|----------------|
| **용도** | 벡터 기반 시각화 | 픽셀 기반 렌더링 |
| **객체에 정보 저장** | 가능 | 불가능 |
| **다시 그리기** | 유리 | 불리 |
| **성능** | 상대적으로 낮음 | 상대적으로 높음 |

- **SVG**: 개별 요소 접근이 필요할 때 적합  
- **Canvas**: 빠른 렌더링이 필요한 대량 데이터 처리에 적합

끝 

### 2장_1절 R 소개

#### 1. R의 탄생
- **R은 오픈소스 프로그램**으로 **통계, 데이터 마이닝, 그래프 생성을 위한 언어**  
- 다양한 최신 통계 분석 및 마이닝 기능을 제공하며, **5,000개 이상의 패키지가 수시로 업데이트됨**

#### 2. 통계분석 도구의 비교

| **구분** | **SAS** | **SPSS** | **오픈소스 R** |
|---------|--------|--------|-------------|
| **프로그램 비용** | 유료, 고가 | 유료, 고가 | 오픈소스 (무료) |
| **설치 환경** | 대용량 | 중소형 | 모듈화된 간단한 구조 |
| **다양한 통계 모델 및 빅데이터 처리** | 빠름 | 보통 | 우수 |
| **최신 알고리즘 및 업데이트** | 느림 | 다소 느림 | 매우 빠름 |
| **분석 작업의 복잡도** | 유료 도서 및 유료 강의 필요 | 유료 도서 및 유료 강의 필요 | 공개 논문과 웹 문서 지원 |



#### 3. R의 특징
- **오픈소스 프로그램**  
- **우수한 그래픽 및 시각화 기능**  
- **다양한 데이터 처리 및 분석 방식 제공**  
- **모든 운영체제에서 사용 가능(윈도우, 맥, 리눅스)**  
- **표준 그래픽(ggplot2) 및 통계 모델 기본 제공**  
- **객체 지향적인 함수형 언어**

### 2장_3절 입력과 출력

#### 1. 데이터 입력과 출력
- **R에서는 텍스트 데이터뿐만 아니라 데이터베이스와 다양한 통계 프로그램에서 작성된 데이터를 불러와 분석 가능**
- **입력 함수**
  - `scan()` : 특정 구분자를 지정하여 입력 가능 (`scan("", what="", sep=",")`)
  - `readLines()` : 한 줄씩 읽기 (`readLines("파일명")`)
  - `cat()` : 문자열 출력 (`cat("텍스트 출력", file="파일명")`)

#### 2. 외부 파일 입력과 출력
##### 1) 파일 읽기
- **고정폭 변수 파일(FWF):** `read.fwf("파일명", width=c(w1,w2...))`
- **공백으로 구분된 파일:** `read.table("파일명", sep="", header=TRUE)`
- **CSV 파일 읽기:** `read.csv("파일명", header=TRUE)`

##### 2) 파일 저장
- **CSV 파일로 저장:** `write.csv(데이터, file="파일명")`

#### 3. 웹 페이지(Web Page)에서 데이터 읽어오기
##### 1) 파일 다운로드
- **웹에서 CSV 다운로드:** `read.csv("http://www.example.com/download/data.csv")`
- **FTP에서 CSV 파일 다운로드:** `read.csv("ftp://ftp.example.com/download/data.csv")`
- **HTML 테이블에서 데이터 읽기:** `readHTMLTable("url")`

### 2장_4절 데이터 구조와 데이터 프레임

#### 1. 데이터 구조의 정의

| **특징** | **벡터** | **리스트** | **데이터 프레임** |
|---------|--------|--------|--------------|
| 원소 자료형 | 동질적 | 이질적 | 이질적 |
| 원소의 위치로 인덱싱 | 가능 | 가능 | 가능 |
| 인덱싱으로 여러 개 원소를 묶어 하위 데이터 생성 | 가능 | 가능 | 가능 |
| 원소들의 이름 부여 | 가능 | 가능 | 가능 |

##### 1) 데이터 구조 유형
- **스칼라(Scalar):** 하나의 값만 가지는 벡터  
- **벡터(Vector):** 동일한 자료형의 1차원 데이터  
- **행렬(Matrix):** 동일한 자료형의 2차원 데이터  
- **배열(Array):** 동일한 자료형의 다차원 데이터  
- **요인(Factor):** 범주형 데이터

#### 2. 리스트 다루기
- 리스트 원소 선택:  
  - `L[[n]]`
  - `L[["name"]]`
  - `L$name`

#### 3. 행렬 다루기
- **행렬 크기 설정:** `dim(vec) <- c(2, 3)`
- **행 이름 설정:**  
  ```r
  rownames(mtrx) <- c("rowname1", "rowname2", ...)
  ```
  
- **열 이름 설정:**  
  열 이름 설정:
  ```r
  colnames(mtrx) <- c("colname1", "colname2", ...)
  ```

### 2장_5절 데이터 변형 1

#### 1. 데이터 구조 변환

| 변환 | 방법 |
|------|------|
| **벡터 → 리스트** | `as.list(vec)` |
| **벡터 → 행렬** | `cbind(vec)`, `as.matrix(vec)` (1열), `rbind(vec)` (1행), `matrix(vec, n, m)` (n × m 행렬) |
| **벡터 → 데이터 프레임** | `as.data.frame(vec)`, `data.frame(vec)` |
| **리스트 → 벡터** | `unlist(lst)` |
| **리스트 → 행렬** | `as.matrix(lst)`, `rbind(lst)`, `matrix(lst, n, m)` |
| **리스트 → 데이터 프레임** | `as.data.frame(lst)` |
| **행렬 → 벡터** | `as.vector(mat)` |
| **행렬 → 리스트** | `as.list(mat)` |
| **행렬 → 데이터 프레임** | `as.data.frame(mat)` |
| **데이터 프레임 → 벡터** | `dfm[[1]]`, `dfm[,1]` |
| **데이터 프레임 → 리스트** | `as.list(dfm)` |
| **데이터 프레임 → 행렬** | `as.matrix(dfm)` |

#### 2. 집단으로 분할하기
- 벡터: `split(vec, fac)`
- 데이터프레임: `split(dfm, fac)`

#### 3. 함수 적용하기
- **벡터 및 행렬:**  
  - `apply(mtr, 1, func)` (행 기준)  
  - `apply(mtr, 2, func)` (열 기준)  
- **리스트:**  
  - `lapply(lst, func)`  
  - `sapply(lst, func)`  
- **데이터 프레임:**  
  - `lapply(dfm, func)`, `sapply(dfm, func)`  
  - `apply(dfm, 1, func)`, `apply(dfm, 2, func)`

#### 4. 집단별 함수 적용하기
- `tapply(vec, fac, func)`
- `by(dfm, fac, func)`

#### 5. 병렬 벡터 및 리스트 함수 적용하기
- 벡터: `mapply(func, vec1, vec2, vec3, ...)`
- 리스트: `mapply(func, lst1, lst2, lst3, ...)`

#### 6. 문자열 처리
##### 1) 문자 길이 및 연결
- 문자 길이: `nchar("문자열")`
- 벡터 길이: `length(vec)`
- 문자열 연결: `paste("단어", "문장", scalar)`

##### 2) 부분 문자열 추출 및 변환
- 부분 문자열 추출: `substr("문자열", 시작번호, 끝번호)`
- 구분자로 분할: `strsplit("문자열", "구분자")`
- 문자열 변경: `sub("대상문자열", "변경문자열", s)`, `gsub("대상문자열", "변경문자열", s)`

#### 7. 날짜 처리
##### 1) 날짜 변환
- 날짜 문자 → 날짜형 변환: `as.Date("2014-12-25")`
- 특정 형식 적용: `as.Date("12/25/2014", format="%m/%d/%Y")`
- 현재 날짜 출력: `format(Sys.Date(), format="%m/%d/%Y")`

##### 2) 날짜 형식 코드
| 코드 | 예제 (R 표현) | 출력 형식 |
|------|-------------|---------|
| `%b` | `format(Sys.Date(), "%b")` | 축약된 월 이름 (`Jan`) |
| `%B` | `format(Sys.Date(), "%B")` | 전체 월 이름 (`January`) |
| `%d` | `format(Sys.Date(), "%d")` | 두 자리 숫자 일 (`31`) |
| `%m` | `format(Sys.Date(), "%m")` | 두 자리 숫자 월 (`12`) |
| `%y` | `format(Sys.Date(), "%y")` | 두 자리 연도 (`14`) |
| `%Y` | `format(Sys.Date(), "%Y")` | 네 자리 연도 (`2014`) |

### 4장_5절 다차원 척도법과 주성분 분석 1

#### 1. 다차원 척도법 (MDS)
##### 1) 정의 및 목적
- **객체들 간의 유사성/비유사성을 측정하여 객체들을 2차원 또는 3차원에 점으로 배치하는 기법**
- **객체의 변수값을 이용하여 2차원 공간상에 점으로 표시하고 시각적으로 관계를 이해하는 것이 목적**

##### 2) 방법

- **객체들 간의 거리 계산:**  
  - 유클리드 거리(Euclidean Distance)를 활용
  - $`
    d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_p - y_p)^2}
    `$
- **STRESS 값 계산:**  
  - 원래 거리와 재구성된 거리의 차이를 최소화하는 방식

##### 3) 다차원 척도법의 종류

| 유형 | 설명 |
|------|------|
| **계량적 MDS (Metric MDS)** | 데이터가 **구간척도 또는 비율척도**로 측정될 때 적용 |
| **비계량적 MDS (Nonmetric MDS)** | **서열척도**의 데이터를 사용하여 거리를 변환하여 시각화 |

### 4장_5절 주성분 분석 (PCA)

##### 1) 정의 및 목적
- 상관관계가 있는 변수들을 결합하여 상관관계가 없는 변수로 변환하는 기법
- **주요 목적**
  - 데이터 차원 축소
  - 선형결합으로 변수 축약
  - 차원 축소를 통해 데이터 해석과 관리 용이

##### 2) 주성분 분석 vs 요인 분석

| 비교 항목 | 주성분 분석 (PCA) | 요인 분석 (FA) |
|----------|----------------|---------------|
| **생성된 변수 수** | 원래 변수 개수와 동일 | 원래 변수보다 적음 |
| **생성된 변수 간의 관계** | 주성분들은 서로 **상관 없음** | 요인들은 서로 상관 있을 수 있음 |
| **목표** | 데이터 **축소 및 요약** | **잠재 요인 발굴** |

##### 3) 주성분의 선택법
- **누적기여율(Cumulative Proportion)** 이 **85% 이상**이면 선택 가능
- **Scree Plot**에서 **고윳값(Eigen Value)** 이 **급격히 감소하기 전 단계까지 선택**

